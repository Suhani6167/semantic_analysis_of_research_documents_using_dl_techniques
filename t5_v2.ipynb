{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "46998388e97049bf9da4ac8f9a2ac332",
            "6eb75272f8ea473fa9c8ea9c0246922f",
            "5a4e9c50dce1427bb4f529a61264a9e6",
            "9739bbc0811f4ab6bb517382835b61e5",
            "f86166a918e148f8a94cb90c84a6b0e2",
            "70eab40fce164367adccfd1af7663a30",
            "55ad16f59f8d4e4c845773706e924131",
            "d23fb01f8d1945acad691cdaec954858",
            "a1d70cabb78b4547bf820ecb307834c0",
            "fa564a314f7c4d1586c689d6bcd9865b",
            "07861c2f2ad845878569f1c02b2a0b1b",
            "ea6ea5e0ebb44c7db15bbdf2c596e0f0",
            "714f1139efe64b04b7383b83278ce848",
            "c8752b07bb654518905783d0fbf406a1",
            "a05332cbae284043ae9fff6ca76013bb",
            "112430cde5604dafa49909faf2662575",
            "d4b47d47f05547a690ecd9c8c6eb7472",
            "be7889662a5f4ea2b5f68800b6332a09",
            "185cfa7df6ff4698bad888e19fbbc730",
            "6433558be09f443991be3d92f46b2389",
            "5ea2f2ba007d4833b7bc9bdd9067551c",
            "0c1009f2c8aa42a5bdfcc47477e0abdd",
            "e2b3aa92590247a99f345b864131848e",
            "2e8b32a739da4340bdb11763e70aa881",
            "2ce80cc81fb54508b681b493e60c039a",
            "23c010d4894048c5b87bdc415652493d",
            "e8fec43a27af4454bea3464c6c3b1d8f",
            "f2fdb265266141e5b77b0855448b166d",
            "cf6a055a513941aea062c046bd6425f6",
            "a451f183441341e2be39d64d395ca0e2",
            "261d05d789fa4acc9e6851d8fcaca0ba",
            "e57a861bf6384786a0074e004e3e04c9",
            "9d60e5493b774d50bea3a0dd09902458",
            "998557bafeaa4b09af2c95ef12d66f2b",
            "13139d9d16c74865bb95cd4f928f6aa3",
            "6c6c357f264749c2b25af984743041af",
            "93d197645aef41b2b4bb5f9960e1f520",
            "5868b37780e943dfa52d03c15459f16d",
            "aaf7dc1b1a124ce78b83811f8dbe2f01",
            "50f7e6c8a084407080f0678ef6b6cdc5",
            "4fdad0235dbf4a289f52325bb99e67c1",
            "9b2bc9db24ce4db2b11ba02c4630e3ad",
            "ba6941730413412085614c1d03de67b7",
            "ba67de76f125460282e76812a71a2ba8",
            "754a7a6d8a8d49c4b2cdb11b9a07a6e0",
            "9eb2ce2630e741718056d323cd33687b",
            "60a6773ba2f242f781d2946de1d84169",
            "f249b59a5009476e967bf122209b4642",
            "c93ea600877144ca8d05e317f3a4ef4b",
            "e14d8759f87b4de6b4c416e03d6299d5",
            "87d7fbc6c98e4d44bcc9566a9786413b",
            "da707172215a404ebc0b4a40b83ff6f5",
            "ca483b91fc044266bdc33a95b531f6cd",
            "dd83c26adb6d417eba57a889e897f0e8",
            "ddacbd8e66674ee380b8507f233021cb"
          ]
        },
        "id": "9IlBYdxBq2Nq",
        "outputId": "b4b3f494-a6ad-4cf2-b288-c5b6d3ce17e4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.21.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.15.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.1.4)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.5)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets) (2024.6.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.10.5)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.24.6)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.2->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "46998388e97049bf9da4ac8f9a2ac332",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ea6ea5e0ebb44c7db15bbdf2c596e0f0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/1.39M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e2b3aa92590247a99f345b864131848e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "998557bafeaa4b09af2c95ef12d66f2b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/892M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "754a7a6d8a8d49c4b2cdb11b9a07a6e0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ğŸ¤— Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='2260' max='2260' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [2260/2260 32:48, Epoch 10/10]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.220100</td>\n",
              "      <td>1.408535</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>1.645100</td>\n",
              "      <td>1.350604</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>1.634200</td>\n",
              "      <td>1.323870</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>1.142900</td>\n",
              "      <td>1.308288</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>1.241400</td>\n",
              "      <td>1.303400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>1.278900</td>\n",
              "      <td>1.296958</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>1.131000</td>\n",
              "      <td>1.297100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>1.204100</td>\n",
              "      <td>1.293723</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>1.492900</td>\n",
              "      <td>1.296238</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>1.064500</td>\n",
              "      <td>1.295750</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "('/content/drive/MyDrive/new dataset/model_t5_v2/tokenizer_config.json',\n",
              " '/content/drive/MyDrive/new dataset/model_t5_v2/special_tokens_map.json',\n",
              " '/content/drive/MyDrive/new dataset/model_t5_v2/spiece.model',\n",
              " '/content/drive/MyDrive/new dataset/model_t5_v2/added_tokens.json')"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "!pip install datasets\n",
        "import os\n",
        "import torch\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration, Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
        "from datasets import Dataset\n",
        "\n",
        "# Set environment variable for debugging\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
        "\n",
        "# Domain-wise input and output directories\n",
        "input_directories = [\n",
        "    \"/content/drive/MyDrive/new dataset/dataset_v2/CLOUD/CLOUD/input\",\n",
        "    \"/content/drive/MyDrive/new dataset/dataset_v2/DL (1)/DL/input\",\n",
        "    \"/content/drive/MyDrive/new dataset/dataset_v2/cybersecurity/cybersecurity/input\",\n",
        "    \"/content/drive/MyDrive/new dataset/dataset_v2/iot/iot/iot/iot/input\",\n",
        "    \"/content/drive/MyDrive/new dataset/dataset_v2/ml/input\"\n",
        "]\n",
        "\n",
        "output_directories = [\n",
        "    \"/content/drive/MyDrive/new dataset/dataset_v2/CLOUD/CLOUD/output\",\n",
        "    \"/content/drive/MyDrive/new dataset/dataset_v2/DL (1)/DL/output\",\n",
        "    \"/content/drive/MyDrive/new dataset/dataset_v2/cybersecurity/cybersecurity/output\",\n",
        "    \"/content/drive/MyDrive/new dataset/dataset_v2/iot/iot/iot/iot/output\",\n",
        "    \"/content/drive/MyDrive/new dataset/dataset_v2/ml/output\"\n",
        "]\n",
        "\n",
        "# Function to load datasets from multiple domains\n",
        "def load_domain_datasets(input_directories, output_directories):\n",
        "    input_texts = []\n",
        "    summaries = []\n",
        "\n",
        "    for input_dir, output_dir in zip(input_directories, output_directories):\n",
        "        input_files = sorted(os.listdir(input_dir))\n",
        "        output_files = sorted(os.listdir(output_dir))\n",
        "\n",
        "        for input_file, output_file in zip(input_files, output_files):\n",
        "            input_file_path = os.path.join(input_dir, input_file)\n",
        "            output_file_path = os.path.join(output_dir, output_file)\n",
        "\n",
        "            with open(input_file_path, 'r', encoding='utf-8') as f:\n",
        "                input_texts.append(f.read())\n",
        "\n",
        "            with open(output_file_path, 'r', encoding='utf-8') as f:\n",
        "                summaries.append(f.read())\n",
        "\n",
        "    return input_texts, summaries\n",
        "\n",
        "# Load datasets from the specified directories\n",
        "input_texts, summaries = load_domain_datasets(input_directories, output_directories)\n",
        "\n",
        "# Load T5 Base Model and Tokenizer\n",
        "model_name = \"t5-base\"  # Updated to \"t5-base\"\n",
        "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
        "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
        "\n",
        "# Tokenize data\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples['input_texts'], truncation=True, padding='max_length', max_length=512)\n",
        "\n",
        "train_encodings = tokenize_function({'input_texts': input_texts})\n",
        "train_labels = tokenize_function({'input_texts': summaries})\n",
        "\n",
        "# Create Dataset\n",
        "dataset = Dataset.from_dict({\n",
        "    'input_ids': train_encodings['input_ids'],\n",
        "    'attention_mask': train_encodings['attention_mask'],\n",
        "    'labels': train_labels['input_ids']\n",
        "})\n",
        "\n",
        "# Split the dataset into 80% training and 20% validation\n",
        "train_test_split = dataset.train_test_split(test_size=0.2)\n",
        "\n",
        "train_dataset = train_test_split['train']\n",
        "eval_dataset = train_test_split['test']\n",
        "\n",
        "# Define training arguments\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir=\"/content/drive/MyDrive/new dataset/model_t5_v2\",  # Specify the directory to save model outputs\n",
        "    per_device_train_batch_size=2,\n",
        "    per_device_eval_batch_size=2,\n",
        "    num_train_epochs=10,  # Run for 10 epochs\n",
        "    logging_dir='./logs',\n",
        "    logging_steps=10,\n",
        "    evaluation_strategy=\"epoch\"\n",
        ")\n",
        "\n",
        "# Initialize Trainer\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,  # Use the 20% of the dataset for evaluation\n",
        "    tokenizer=tokenizer\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "trainer.train()\n",
        "\n",
        "# Save the model and tokenizer\n",
        "model.save_pretrained(\"/content/drive/MyDrive/new dataset/model_t5_v2\")\n",
        "tokenizer.save_pretrained(\"/content/drive/MyDrive/new dataset/model_t5_v2\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ysaGmQKWgXw"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5kEBhbf_65Pd",
        "outputId": "1d19a138-614e-4002-ba28-45c82ba8421e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vhVv_OTM6qef",
        "outputId": "6e3d5be7-27cb-4540-b03b-967e974019d8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (0.34.2)\n",
            "Collecting pdfplumber\n",
            "  Downloading pdfplumber-0.11.4-py3-none-any.whl.metadata (41 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m42.0/42.0 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: transformers[torch] in /usr/local/lib/python3.10/dist-packages (4.44.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (3.16.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.24.6)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (4.66.5)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.4.0+cu121)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Collecting pdfminer.six==20231228 (from pdfplumber)\n",
            "  Downloading pdfminer.six-20231228-py3-none-any.whl.metadata (4.2 kB)\n",
            "Requirement already satisfied: Pillow>=9.1 in /usr/local/lib/python3.10/dist-packages (from pdfplumber) (9.4.0)\n",
            "Collecting pypdfium2>=4.18.0 (from pdfplumber)\n",
            "  Downloading pypdfium2-4.30.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (48 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m48.5/48.5 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six==20231228->pdfplumber) (3.3.2)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six==20231228->pdfplumber) (43.0.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers[torch]) (2024.6.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers[torch]) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (1.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (3.1.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2024.8.30)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber) (1.17.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->transformers[torch]) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->transformers[torch]) (1.3.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber) (2.22)\n",
            "Downloading pdfplumber-0.11.4-py3-none-any.whl (59 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m59.2/59.2 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pdfminer.six-20231228-py3-none-any.whl (5.6 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m57.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pypdfium2-4.30.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.8 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m68.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pypdfium2, pdfminer.six, pdfplumber\n",
            "Successfully installed pdfminer.six-20231228 pdfplumber-0.11.4 pypdfium2-4.30.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/models/t5/tokenization_t5.py:289: UserWarning: This sequence already has </s>. In future versions this behavior may lead to duplicated eos tokens being added.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Combined Short Summary:\n",
            " This research investigates the use of Bayesian Network Model (BNM) to estimate the presence of estrous cycle in Japanese dairy cattle. Through the experiment with 280 Japanese anestrus Holstein dairy cows, it is found that the estimation for finding out estrous cycle represents almost 55% accuracy while considering all samples. On the contrary, almost 73% accurate estimation could be achieved while using suspended likelihood in sample datasets. The proposed model has more confidence than the estimation accuracy lies The estrous cycle of cattle is the period from one estrus (heat, phase of sexual receptivity) to the next estrus. For the cow and heifer, this period averages 21 days, with a typical range of 18 to 24 days in length [1, 2, 3, 4]. The estrous cycle is the period from one estrus (heat, phase of sexual receptivity) to the next estrus, with a typical range This study aims to find out the optimum factors to have an estrous cycle of bovine using a Bayesian Network Model (BNM) analysis in Japanese dairy industries. 280 different Japanese Holstein cows observing with their (2.0 to 3.25), postpartum interval, and parity numbers to discover the ideal timing for artificial insemination to make them pregnant. All these 280 samples found anestrus in their farm. The study highlights that, The paper presents a Bayesian network model for identifying estrous cycle using the data sets. The model is designed with the prior and conditional probability of each parameter, and the posterior probability of having estrous cycle or not is determined using the Bayesian equation. The proposed method uses a directed graph, the parent node indicates the cause, and the child node indicates the result. The system finds its accuracy toward the mid-range scores (2.50 to 4.00 This study investigates the estrous cycle in cows by observing 280 individual cows from the dairy farm of Iwate Prefecture, Japan. The study focuses on the postpartum interval, which is the period of temporary infertility known as postpartum anestrus. Usually, cattle do not have estrous cycle during this period, which is known as postpartum anestrus. This research introduces or days after calving parameter The research investigated 280 dairy cows from Iwate Prefecture with the cooperation of Morinaga Dairy Service () Co. Ltd., Japan. The cattle were identified with a unique number in the farm and then observed by an experienced inspector. The required,, Parity number and estrous cycle related data were collected and processed according to the proposed Bayesian Network Model (Fig.3). The 280 individual cattle were found anestrus due to some The learning of Bayesian network includes 270 individual data and the rest of 10 data used for test purpose. The 10 data leave out approach improves the accuracy of the model and 27 individual sets of learning and testing data sets used to validate the proposed model. The Bayesian tree was formed by using the estimation of, and Parity number of each cow. The posterior probability was calculated under these three evidences by following (1). The overall optimum factors for the presence of estrous This research investigated the estimation accuracy of Bayesian network models with higher confidence. The results showed that, when the likelihood is 0.5, the highest average estimation accuracy for the presence of estrous cycle is almost 73% (total 40 cattle) and the lowest is 50% when the likelihood is considering 0.7. However, most of the dairy cows lies within the likelihood of 0.6 (130 + 40 = 170 cattle) and the average estimation accuracy for considering all cattle This research presents the discovering method of cattleâ€™s estrous cycle presence using a new approach of Bayesian network model. The inclusion of body condition parameters, postpartum intervals and parity in the model helped to evaluate more accurate objective estimation of estrous cycle presence in cattle. The results and analysis confirmed that, the more accurate and optimum factors for cattle productivity could be found by the proposed method. The authors believe, the objective estimation definitely boosts up the productivity in this [3] J. Walker, and G. Perry, â€œThe Estrous cycle of Cattleâ€, Mississippi State University in cooperation with Department of Agriculture, Publication No.2616, 2010. [4] L. F. Pfeifer, S.C.B.S. Leal, A. Scheneider, E. Schemitt, and M.N. Correa, â€œEffect of ovulatory follicle diameter Estrous cycle is the period between one estrus (heat, phase of sexual receptivity) and the next estrus (heat, phase of sexual receptivity). The estrous cycle of cattle is the period between one estrus (heat, phase of sexual receptivity) and the next estrus (heat, phase of sexual receptivity). It is characterized by 21 days, with a typical range of 18 to 24 days This study investigates the optimum factors to have an estrous cycle of bovine using a Bayesian Network Model (BNM) in Japanese dairy industries. 280 different Japanese Holstein cows observing with their (2.0 to 3.25), postpartum interval, and parity numbers to discover the ideal timing for artificial insemination to make them pregnant. The study highlights that, all these 280 samples found anestrus in their farm. The study highlights the The paper presents a Bayesian network tree structure for identifying estrous cycle in cattle using the data sets. The proposed Bayesian network model is designed with each parameterâ€™s prior and conditional probability. The proposed method uses the Bayesian equation for each parameter as follows: ( | ) Prior probability of having estrous cycle or not; ( | ) conditional probability of having estrous cycle or not based on information; ( | ) probability of evidence The study investigates the estrous cycle of 280 individual 4.0 cattle of Morinaga Dairy Service Co. Ltd. (), Japan. The method (0.25 increase) have good repeatability across 4.0 and within observers, including simplified body scoring as well as higher value as diagnostic test [24]. The research introduces as an individual parameter in the proposed, which assists to find more accurate estimation of estrous cycle with sample data. The following table describes and categories the of The research investigated 280 individual Holstein cattle, which were found anestrus due to some health hazards in its own farm. The cattle were identified with a unique number in the Iwate Prefecture with the cooperation of Morinaga Dairy Service () Co. Ltd., Japan. The highest number of parity for each cow is 9 and the lowest is 1. The cattle were observed in accordance with the method of Ferguson [24] and then observed by an The proposed model uses Bayesian network with 270 individual data and the rest of 10 data used for test purpose. The 10 data leave out approach improves the accuracy of the model and 27 individual sets of learning and testing data sets used to validate the model. When the model acquired higher confidence (less entropy value), the estimation accuracy for the presence of estrous cycle lies in between 93 to 100%. The overall highest estimation accuracy for finding estrous cycle based on proposed The research 34 | a ge.thesai.org ( discovered that, the sample data might not be enough to satisfy the proposed Bayesian model. Therefore, the entropy of proposed modelâ€™s outputs was calculated to achieve reliable discrimination and use it for discrimination-suspension rule. The entropy is the risk of incorrect discrimination and if entropy exceeds some predefined threshold, the discrimination could be This research presents the discovering method of cattleâ€™s estrous cycle presence using a new approach of Bayesian network model. The inclusion of body condition parameters, postpartum intervals and parity in the model helped to evaluate more accurate objective estimation of estrous cycle presence in cattle. The results show that, when the model gets higher confidence (0.15 entropy>0.15), the accuracy rate lies in between 93% to 100%, which is one of the most [2] J. A. Parish, J. E. Larson, and R. C. Vann, â€œThe Estrous cycle of Cattleâ€, Mississippi State University in cooperation with Department of Agriculture, Publication No.2616, 2010. [3] G. Perry, â€œThe Bovine Estrous Cycle- â€, South Dakota State University-Cooperative Extensive Service, Accessed December, 2015. The study was funded and supported by Ministry of Agriculture, Forestry\n"
          ]
        }
      ],
      "source": [
        "# Install necessary libraries\n",
        "!pip install transformers[torch] accelerate -U pdfplumber\n",
        "\n",
        "# Import libraries\n",
        "import pdfplumber\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "import torch\n",
        "import re\n",
        "\n",
        "# Load the model and tokenizer\n",
        "model_path = \"/content/drive/MyDrive/new dataset/model_t5_v2\"  # Ensure the model path is correct\n",
        "tokenizer = T5Tokenizer.from_pretrained(model_path)\n",
        "model = T5ForConditionalGeneration.from_pretrained(model_path)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "# Function to extract text from multi-column PDF\n",
        "def extract_text_from_columns(pdf_path):\n",
        "    text = \"\"\n",
        "    with pdfplumber.open(pdf_path) as pdf:\n",
        "        for page in pdf.pages:\n",
        "            # Define bounding boxes for left and right columns\n",
        "            left_bbox = (0, 0, page.width / 2, page.height)\n",
        "            right_bbox = (page.width / 2, 0, page.width, page.height)\n",
        "\n",
        "            # Extract text from left column\n",
        "            left_text = page.within_bbox(left_bbox).extract_text()\n",
        "\n",
        "            # Extract text from right column\n",
        "            right_text = page.within_bbox(right_bbox).extract_text()\n",
        "\n",
        "            # Combine text from both columns\n",
        "            combined_text = (left_text or '') + ' ' + (right_text or '')\n",
        "            text += combined_text + '\\n'  # Adding a newline for separation\n",
        "    return text\n",
        "\n",
        "# Function to clean text by removing headers, footers, and references\n",
        "def clean_text(text):\n",
        "    # Remove headers and footers (assuming they are at the start and end of pages)\n",
        "    cleaned_text = re.sub(r'\\b(?:[A-Z][A-Z0-9 ]+|Page \\d+|Header|Footer)\\b', '', text, flags=re.MULTILINE)\n",
        "\n",
        "    # Remove references section (assuming it starts with \"References\" or similar)\n",
        "    cleaned_text = re.sub(r'\\bReferences\\b.*$', '', cleaned_text, flags=re.S)\n",
        "\n",
        "    return cleaned_text\n",
        "\n",
        "# Function to extract sections from the cleaned text\n",
        "def extract_section(text, section_title):\n",
        "    # Regex to find sections based on title and extract the following text until the next section\n",
        "    pattern = rf'{section_title}[\\s\\S]*?(?=\\n[A-Z][A-Z\\s]+:|$)'\n",
        "    match = re.search(pattern, text, re.IGNORECASE)\n",
        "    if match:\n",
        "        return match.group().strip()\n",
        "    return ''\n",
        "\n",
        "# Function to chunk text and generate summaries\n",
        "def chunk_text(text, max_length=512):\n",
        "    tokens = tokenizer.encode(text, truncation=False)\n",
        "    chunks = [tokens[i:i + max_length] for i in range(0, len(tokens), max_length)]\n",
        "    return chunks\n",
        "\n",
        "def generate_short_summary(text_chunk):\n",
        "    inputs = tokenizer(text_chunk, return_tensors=\"pt\", max_length=512, truncation=True, padding=\"longest\")\n",
        "    inputs = {key: value.to(device) for key, value in inputs.items()}\n",
        "    summary_ids = model.generate(\n",
        "        inputs[\"input_ids\"],\n",
        "        max_length=100,  # Adjust for shorter summaries\n",
        "        min_length=30,\n",
        "        length_penalty=2.0,\n",
        "        num_beams=4,\n",
        "        early_stopping=True\n",
        "    )\n",
        "    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "    return summary\n",
        "\n",
        "# Function to summarize large text and generate short summary\n",
        "def summarize_large_text(text):\n",
        "    chunks = chunk_text(text)\n",
        "\n",
        "    # Generate short summaries for each chunk\n",
        "    short_summaries = [generate_short_summary(tokenizer.decode(chunk)) for chunk in chunks]\n",
        "\n",
        "    # Combine chunks into one short summary\n",
        "    combined_short_summary = ' '.join(short_summaries)\n",
        "\n",
        "    return combined_short_summary\n",
        "\n",
        "# Process PDF and generate summaries for specific sections\n",
        "pdf_path = \"/content/drive/MyDrive/I/TESTING PAPERS/ML_ 101.pdf\"\n",
        "document_text = extract_text_from_columns(pdf_path)\n",
        "cleaned_text = clean_text(document_text)\n",
        "\n",
        "# Extract sections\n",
        "abstract_text = extract_section(cleaned_text, 'Abstract')\n",
        "results_text = extract_section(cleaned_text, 'Results')\n",
        "methodology_text = extract_section(cleaned_text, 'Methodology')\n",
        "conclusion_text = extract_section(cleaned_text, 'Conclusion')\n",
        "\n",
        "# Combine the extracted sections into a single text\n",
        "combined_text = f\"{abstract_text}\\n{results_text}\\n{methodology_text}\\n{conclusion_text}\"\n",
        "\n",
        "# Generate combined short summary\n",
        "combined_short_summary = summarize_large_text(combined_text)\n",
        "\n",
        "# Print the combined short summary\n",
        "print(\"\\nCombined Short Summary:\\n\", combined_short_summary)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nGDM3-m48CD9"
      },
      "source": [
        "bart"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 942
        },
        "id": "uX1uCws28DBm",
        "outputId": "a088c0b7-8367-4b67-d003-f1d1877a96eb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (2.14.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.15)\n",
            "Requirement already satisfied: fsspec>=2021.11.1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.11.1->datasets) (2025.3.2)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.15)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.33.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.6.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.20.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (4.14.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (1.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->datasets) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->datasets) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->datasets) (2025.7.14)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
          ]
        },
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/drive/MyDrive/new dataset/dataset_v2/CLOUD/CLOUD/input'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1-2185791450.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;31m# Load datasets from the specified directories\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m \u001b[0minput_texts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msummaries\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_domain_datasets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_directories\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_directories\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;31m# Load BART Base Model and Tokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1-2185791450.py\u001b[0m in \u001b[0;36mload_domain_datasets\u001b[0;34m(input_directories, output_directories)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0minput_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_dir\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_directories\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_directories\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         \u001b[0minput_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m         \u001b[0moutput_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/new dataset/dataset_v2/CLOUD/CLOUD/input'"
          ]
        }
      ],
      "source": [
        "!pip install datasets\n",
        "import os\n",
        "import torch\n",
        "from transformers import BartTokenizer, BartForConditionalGeneration, Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
        "from datasets import Dataset\n",
        "\n",
        "# Set environment variable for debugging\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
        "\n",
        "# Domain-wise input and output directories\n",
        "input_directories = [\n",
        "    \"/content/drive/MyDrive/new dataset/dataset_v2/CLOUD/CLOUD/input\",\n",
        "    \"/content/drive/MyDrive/new dataset/dataset_v2/DL (1)/DL/input\",\n",
        "    \"/content/drive/MyDrive/new dataset/dataset_v2/cybersecurity/cybersecurity/input\",\n",
        "    \"/content/drive/MyDrive/new dataset/dataset_v2/iot/iot/iot/iot/input\",\n",
        "    \"/content/drive/MyDrive/new dataset/dataset_v2/ml/input\"\n",
        "]\n",
        "\n",
        "output_directories = [\n",
        "    \"/content/drive/MyDrive/new dataset/dataset_v2/CLOUD/CLOUD/output\",\n",
        "    \"/content/drive/MyDrive/new dataset/dataset_v2/DL (1)/DL/output\",\n",
        "    \"/content/drive/MyDrive/new dataset/dataset_v2/cybersecurity/cybersecurity/output\",\n",
        "    \"/content/drive/MyDrive/new dataset/dataset_v2/iot/iot/iot/iot/output\",\n",
        "    \"/content/drive/MyDrive/new dataset/dataset_v2/ml/output\"\n",
        "]\n",
        "\n",
        "# Function to load datasets from multiple domains\n",
        "def load_domain_datasets(input_directories, output_directories):\n",
        "    input_texts = []\n",
        "    summaries = []\n",
        "\n",
        "    for input_dir, output_dir in zip(input_directories, output_directories):\n",
        "        input_files = sorted(os.listdir(input_dir))\n",
        "        output_files = sorted(os.listdir(output_dir))\n",
        "\n",
        "        for input_file, output_file in zip(input_files, output_files):\n",
        "            input_file_path = os.path.join(input_dir, input_file)\n",
        "            output_file_path = os.path.join(output_dir, output_file)\n",
        "\n",
        "            with open(input_file_path, 'r', encoding='utf-8') as f:\n",
        "                input_texts.append(f.read())\n",
        "\n",
        "            with open(output_file_path, 'r', encoding='utf-8') as f:\n",
        "                summaries.append(f.read())\n",
        "\n",
        "    return input_texts, summaries\n",
        "\n",
        "# Load datasets from the specified directories\n",
        "input_texts, summaries = load_domain_datasets(input_directories, output_directories)\n",
        "\n",
        "# Load BART Base Model and Tokenizer\n",
        "model_name = \"facebook/bart-base\"  # Updated to \"facebook/bart-base\"\n",
        "tokenizer = BartTokenizer.from_pretrained(model_name)\n",
        "model = BartForConditionalGeneration.from_pretrained(model_name)\n",
        "\n",
        "# Tokenize data\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples['input_texts'], truncation=True, padding='max_length', max_length=512)\n",
        "\n",
        "train_encodings = tokenize_function({'input_texts': input_texts})\n",
        "train_labels = tokenize_function({'input_texts': summaries})\n",
        "\n",
        "# Create Dataset\n",
        "dataset = Dataset.from_dict({\n",
        "    'input_ids': train_encodings['input_ids'],\n",
        "    'attention_mask': train_encodings['attention_mask'],\n",
        "    'labels': train_labels['input_ids']\n",
        "})\n",
        "\n",
        "# Split the dataset into 80% training and 20% validation\n",
        "train_test_split = dataset.train_test_split(test_size=0.2)\n",
        "\n",
        "train_dataset = train_test_split['train']\n",
        "eval_dataset = train_test_split['test']\n",
        "\n",
        "# Define training arguments\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir=\"/content/drive/MyDrive/new dataset/model_bart_v2\",  # Specify the directory to save model outputs\n",
        "    per_device_train_batch_size=2,\n",
        "    per_device_eval_batch_size=2,\n",
        "    num_train_epochs=10,  # Run for 10 epochs\n",
        "    logging_dir='./logs',\n",
        "    logging_steps=10,\n",
        "    evaluation_strategy=\"epoch\"\n",
        ")\n",
        "\n",
        "# Initialize Trainer\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,  # Use the 20% of the dataset for evaluation\n",
        "    tokenizer=tokenizer\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "trainer.train()\n",
        "\n",
        "# Save the model and tokenizer\n",
        "model.save_pretrained(\"/content/drive/MyDrive/new dataset/model_bart_v2\")\n",
        "tokenizer.save_pretrained(\"/content/drive/MyDrive/new dataset/model_bart_v2\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DbFcknH1ujJw"
      },
      "source": [
        "**Code of rouge of t5**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pj58_Q0MuhPa",
        "outputId": "d9229c2d-7ed6-42c6-b51e-f22ff4f2a20b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting rouge-score\n",
            "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from rouge-score) (1.4.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from rouge-score) (3.8.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from rouge-score) (1.26.4)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from rouge-score) (1.16.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->rouge-score) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->rouge-score) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->rouge-score) (2024.5.15)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk->rouge-score) (4.66.5)\n",
            "Building wheels for collected packages: rouge-score\n",
            "  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24935 sha256=5b6a836993771fdd46d6ec72903dc727a5cc1f76969b478e9c330f9290f85ca3\n",
            "  Stored in directory: /root/.cache/pip/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\n",
            "Successfully built rouge-score\n",
            "Installing collected packages: rouge-score\n",
            "Successfully installed rouge-score-0.1.2\n",
            "Requirement already satisfied: pdfplumber in /usr/local/lib/python3.10/dist-packages (0.11.4)\n",
            "Requirement already satisfied: pdfminer.six==20231228 in /usr/local/lib/python3.10/dist-packages (from pdfplumber) (20231228)\n",
            "Requirement already satisfied: Pillow>=9.1 in /usr/local/lib/python3.10/dist-packages (from pdfplumber) (9.4.0)\n",
            "Requirement already satisfied: pypdfium2>=4.18.0 in /usr/local/lib/python3.10/dist-packages (from pdfplumber) (4.30.0)\n",
            "Requirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six==20231228->pdfplumber) (3.3.2)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six==20231228->pdfplumber) (43.0.1)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber) (1.17.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber) (2.22)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/models/t5/tokenization_t5.py:289: UserWarning: This sequence already has </s>. In future versions this behavior may lead to duplicated eos tokens being added.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Combined Short Summary:\n",
            " The paper addresses the challenge of identifying and classifying genuine user profiles on online social networks (OSNs) as genuine or not genuine categories. The proposed model achieved an average accuracy of 94% in the classification task considering all three datasets. The results indicate that the proposed model achieved an average accuracy of 94% in the classification task considering all three datasets. Future work will focus on improving the classification of genuine user profiles on online social networks (OSNs). Social networking sites have become increasingly popular due to the rapid growth in their use. However, there are some problems with these sites, such as fake profiles and online impersonation. We need an automatic fake profile detection system to make the people using social networking sites feel safe. These are profiles of people or organizations that donâ€™t actually exist or they impersonate other people. The reasons for creating fake profiles can be to obtain personal information and use it for the hackerâ€™s gain. The paper explores the classification of Genuine user profiles in online social media and networks using three datasets: facebook, instagram, and twitter. The study focuses on three algorithms: Random forest, Neural Network, and Deep Neural Networks. For detecting fake profiles and classifying them as genuine or fake, the authors used various algorithms like Support vector machine, neural network, Deep Neural Networks, and Random Forest. The results show that Random forest achieves higher accuracy The paper presents a framework for identifying fake accounts in social networks using Random Forest Classifier. The dataset used by Barracuda Labs has 922 profiles which are used for training and 240 profiles can be used for testing. Linear, Logistic Regression and Medium Gaussian were used. The dataset was downloaded from GitHub repository and the classifiers were selected. Comparison analysis shows that Medium Gaussian has the highest accuracy of 97.6%. The study consists of 3000 unique users with 1500 being fake accounts and remaining 1500 being normal accounts. The dataset used consists of 3000 unique users with 1500 being fake accounts and remaining 1500 being normal accounts. The results obtained from the DeepProfile approach are compared with Bernoulli, Gradient Boosting, Logistic Regression and which shows that the DeepProfile method gets better accuracy with a smaller loss. An improvement over the previous methods is achieved when the The paper introduces a new machine learning algorithm to detect whether the twitter accounts are genuine or not. The algorithm was compared with Naive Bayes, support vector machines, and neural networks. The proposed algorithm achieved an accuracy of 98% even though it takes less features. The results showed that eighteen out of twenty clones were detected. The work can be extended by considering the tweets using techniques. The experiment used Facebook, Instagram and Twitter datasets from Kaggle. The Instagram dataset has 698 rows of data out of which 120 rows are used for testing. The Facebook dataset has 2820 rows of data out of which twenty percent is used for testing purposes. We used eleven thousand four hundred and twenty tweets from real users and five thousand seven hundred and eighty nine tweets from spam users. The Instagram dataset has 698 rows of data out of which 120 rows are used for The experiment was conducted using the Windows 10 operating system with 4. Dataset in.csv extension was used to upload the.csv files, pre-process and run the model. The dataset used for training and testing consists of 80% of data, 82% of data is used for training and the remaining is used for testing purposes. For the Twitter dataset, 75% of data is used for training and the remaining is used for testing purposes. The results show that Random Forest Random Forest achieves the highest accuracy of 96% for the Instagram dataset compared to other algorithms. For the Twitter dataset, the Random Forest algorithm achieves the highest accuracy of 96%. The Experiment was evaluated using evaluation metrics like recall, precision, accuracy and -score. The Experiment was evaluated using the following formula: Recall.91 0.90 90% 0.90 Neural Network 0.94 0.93 93% 0.93 Random Forest 0.95 0.94 94% 0. This paper proposes a classification model to detect genuine and not genuine users on social media platforms. The paper compares various classification algorithms like, Neural Network and Random Forest on Instagram, Facebook and Twitter datasets. The results show that Random forest achieves the highest accuracy with 95%, while Neural Network achieves the lowest accuracy with 92%. The model can be tested against performance measures other than accuracy and also tested for other datasets. Fig. 5. Comparing the accuracy b&amp; Raja Suzana Raja Kassim, â€œOnline Social Networking for Quality of Lifeâ€, SciVerse Science direct, Procedia â€“ Social and Behavioral Sciences, 713 â€“ 718, 2012. Online social networking sites are vital in the lives of people who make use of the internet to do their daily interests. They are platforms that people use to make friends, build social communities, and connect virtually with people who share similar interests. Using machine learning (ML) algorithms to detect fake profiles on social networking sites is crucial for preventing cybercrimes. Fake profiles are profiles of people or organizations that donâ€™t actually exist or they impersonate other people. They can be either created by humans or computers. Some bots are known as Sybil accounts, while others are known as Sybil accounts. The purpose of creating fake profiles is to obtain personal information and use it for the hackerâ€™s gain. The paper explores the use of three different datasets like facebook, instagram and twitter profiles for the classification of Genuine user profiles in online social media and networks. Using Classifiers like Linear, Logistic Regression and Medium Gaussian, the proposed algorithm achieves higher accuracy in all three datasets compared with other algorithms. The dataset was downloaded from GitHub repository to evaluate the proposed algorithm. Comparison analysis shows that Medium Gaussian has the highest accuracy of The paper introduces a DeepProfile deep neural network (DNN) to detect fake accounts in OSNs. The dataset used consists of 3000 unique users with 1500 being fake accounts and remaining 1500 being normal accounts. Gradient boosting was successful in detecting fake accounts due to strict privacy settings. The proposed DeepProfile algorithm is compared with Bernoulli, Gradient Boosting, and Logistic Regression. The results show that the DeepProfile The study evaluated the accuracy of various classifiers using 10-fold cross validation. Random forest obtained 94% accuracy which was operated on 19 features. Twenty three features were required to get an accuracy of ninety one to ninety two percent for the Naive Bayes algorithm. The decision tree algorithm required only ten features to get an accuracy of 98%. In order to verify the performance of the classifiers, a cross validation of 8 and 10-folds were used The paper presents the Genuine user profile classification model for Facebook, Instagram and Twitter users. The dataset used is refined by pre-processing and extraction of relevant features. The final refined dataset is fed into the classification model that classifies the user profile as genuine or not genuine. The accuracy of ninety percent was achieved by the proposed model. The results indicate that eighteen out of twenty clones were detected. The research can be extended by considering the tweets using techniques. The The data set used for the experiment was a Windows 10 operating system with 4. The dataset was collected, preprocessed and pre-processed using a.csv extension. The dataset was analyzed using a.csv file, preprocessed and run using a.csv extension. The dataset was analyzed using a.csv file and the results were compared to other datasets. The results Table with description The Experiment was conducted using different classifiers like Random Forest and Neural network algorithms. In the Facebook, Instagram and Twitter datasets, there are a total twelve attributes which are depicted in Table with description. For the Facebook dataset 80% of data is used for training and the remaining is used for testing purposes. For the Twitter dataset 75% of data is used for training and the remaining is used for testing purposes. For the Twitter dataset, seven attributes Fig. 3. Comparing the accuracy of classifiers for all 3 types of datasets. Random forest gets 94% accuracy for Facebook dataset as depicted in Fig 3. For Instagram dataset set 1 gets the highest accuracy of 96%. Classifier Precision -score Accuracy Recall.91 0.90 90% 0.90 Neural Network 0.94 0.93 93% 0.93 Random Forest 0.95 0.94 94% 0.94 Fig. 4. Comparing the accuracy of This paper proposes a classification model to detect genuine and not genuine users on social media. The paper compares various classification algorithms like, Neural Network and Random forest on Instagram, Facebook and Twitter datasets. The results show that Random forest performed better with accuracy 95%. This algorithm is used as the highest accuracy classifier in the proposed model. The model can be tested against performance measures other than accuracy and also tested for other datasets.\n",
            "\n",
            "ROUGE Scores:\n",
            " {'rouge1': Score(precision=0.09121171770972038, recall=0.8954248366013072, fmeasure=0.16555891238670697), 'rouge2': Score(precision=0.06129247168554297, recall=0.6052631578947368, fmeasure=0.11131276467029642), 'rougeL': Score(precision=0.0659121171770972, recall=0.6470588235294118, fmeasure=0.11963746223564956)}\n"
          ]
        }
      ],
      "source": [
        "# Import libraries\n",
        "\n",
        "!pip install rouge-score\n",
        "!pip install pdfplumber\n",
        "import pdfplumber\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "import torch\n",
        "import re\n",
        "from rouge_score import rouge_scorer\n",
        "\n",
        "# Load the model and tokenizer\n",
        "model_path = \"/content/drive/MyDrive/new dataset/model_t5_v2\"  # Ensure the model path is correct\n",
        "tokenizer = T5Tokenizer.from_pretrained(model_path)\n",
        "model = T5ForConditionalGeneration.from_pretrained(model_path)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "# Function to extract text from multi-column PDF\n",
        "def extract_text_from_columns(pdf_path):\n",
        "    text = \"\"\n",
        "    with pdfplumber.open(pdf_path) as pdf:\n",
        "        for page in pdf.pages:\n",
        "            # Define bounding boxes for left and right columns\n",
        "            left_bbox = (0, 0, page.width / 2, page.height)\n",
        "            right_bbox = (page.width / 2, 0, page.width, page.height)\n",
        "\n",
        "            # Extract text from left column\n",
        "            left_text = page.within_bbox(left_bbox).extract_text()\n",
        "\n",
        "            # Extract text from right column\n",
        "            right_text = page.within_bbox(right_bbox).extract_text()\n",
        "\n",
        "            # Combine text from both columns\n",
        "            combined_text = (left_text or '') + ' ' + (right_text or '')\n",
        "            text += combined_text + '\\n'  # Adding a newline for separation\n",
        "    return text\n",
        "\n",
        "# Function to clean text by removing headers, footers, and references\n",
        "def clean_text(text):\n",
        "    # Remove headers and footers (assuming they are at the start and end of pages)\n",
        "    cleaned_text = re.sub(r'\\b(?:[A-Z][A-Z0-9 ]+|Page \\d+|Header|Footer)\\b', '', text, flags=re.MULTILINE)\n",
        "\n",
        "    # Remove references section (assuming it starts with \"References\" or similar)\n",
        "    cleaned_text = re.sub(r'\\bReferences\\b.*$', '', cleaned_text, flags=re.S)\n",
        "\n",
        "    return cleaned_text\n",
        "\n",
        "# Function to extract sections from the cleaned text\n",
        "def extract_section(text, section_title):\n",
        "    # Regex to find sections based on title and extract the following text until the next section\n",
        "    pattern = rf'{section_title}[\\s\\S]*?(?=\\n[A-Z][A-Z\\s]+:|$)'\n",
        "    match = re.search(pattern, text, re.IGNORECASE)\n",
        "    if match:\n",
        "        return match.group().strip()\n",
        "    return ''\n",
        "\n",
        "# Function to chunk text and generate summaries\n",
        "def chunk_text(text, max_length=512):\n",
        "    tokens = tokenizer.encode(text, truncation=False)\n",
        "    chunks = [tokens[i:i + max_length] for i in range(0, len(tokens), max_length)]\n",
        "    return chunks\n",
        "\n",
        "def generate_short_summary(text_chunk):\n",
        "    inputs = tokenizer(text_chunk, return_tensors=\"pt\", max_length=512, truncation=True, padding=\"longest\")\n",
        "    inputs = {key: value.to(device) for key, value in inputs.items()}\n",
        "    summary_ids = model.generate(\n",
        "        inputs[\"input_ids\"],\n",
        "        max_length=100,  # Adjust for shorter summaries\n",
        "        min_length=30,\n",
        "        length_penalty=2.0,\n",
        "        num_beams=4,\n",
        "        early_stopping=True\n",
        "    )\n",
        "    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "    return summary\n",
        "\n",
        "# Function to summarize large text and generate short summary\n",
        "def summarize_large_text(text):\n",
        "    chunks = chunk_text(text)\n",
        "\n",
        "    # Generate short summaries for each chunk\n",
        "    short_summaries = [generate_short_summary(tokenizer.decode(chunk)) for chunk in chunks]\n",
        "\n",
        "    # Combine chunks into one short summary\n",
        "    combined_short_summary = ' '.join(short_summaries)\n",
        "\n",
        "    return combined_short_summary\n",
        "\n",
        "# Function to calculate ROUGE score\n",
        "def calculate_rouge(predicted_summary, reference_summary):\n",
        "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
        "    scores = scorer.score(reference_summary, predicted_summary)\n",
        "    return scores\n",
        "\n",
        "# Process PDF and generate summaries for specific sections\n",
        "pdf_path = \"/content/drive/MyDrive/CONIT2022Paper0627.pdf\"\n",
        "document_text = extract_text_from_columns(pdf_path)\n",
        "cleaned_text = clean_text(document_text)\n",
        "\n",
        "# Extract sections\n",
        "abstract_text = extract_section(cleaned_text, 'Abstract')\n",
        "results_text = extract_section(cleaned_text, 'Results')\n",
        "methodology_text = extract_section(cleaned_text, 'Methodology')\n",
        "conclusion_text = extract_section(cleaned_text, 'Conclusion')\n",
        "\n",
        "# Combine the extracted sections into a single text\n",
        "combined_text = f\"{abstract_text}\\n{results_text}\\n{methodology_text}\\n{conclusion_text}\"\n",
        "\n",
        "# Generate combined short summary\n",
        "combined_short_summary = summarize_large_text(combined_text)\n",
        "\n",
        "# Print the combined short summary\n",
        "print(\"\\nCombined Short Summary:\\n\", combined_short_summary)\n",
        "\n",
        "# Reference summary (you would provide the actual reference summary for comparison)\n",
        "reference_summary = \"\"\"\n",
        "  identifying a genuine user profile on social media has gained significant importance\n",
        "in the lieu of detecting social media users from cyber criminals.\n",
        "With this regard, this paper is focussed at developing a\n",
        "machine learning model that identifies and classifies user\n",
        "profiles as genuine or not genuine categories. Three different datasets are considered\n",
        "like facebook, instagram and twitter profiles for the\n",
        "classification of Genuine user profiles in online social media\n",
        "and networks. The three effective techniques discussed here\n",
        "are SVM, Random forest and Neural Network. From the\n",
        "above algorithms, Random forest achieves higher accuracy\n",
        "in all three datasets compared with other algorithms.Instagram dataset highest accuracy is achieved which is\n",
        "96%. Also Random forest gets an average accuracy of 95%\n",
        "considering all three datasets.The Comparison result of various\n",
        "classification algorithms like SVM, Neural Network and\n",
        "Random forest on Instagram, Facebook and Twitter datasets\n",
        "highlights that Random forest performed better with\n",
        "accuracy 95%.\n",
        "\"\"\"\n",
        "\n",
        "# Calculate ROUGE scores\n",
        "rouge_scores = calculate_rouge(combined_short_summary, reference_summary)\n",
        "\n",
        "# Print the ROUGE scores\n",
        "print(\"\\nROUGE Scores:\\n\", rouge_scores)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ti5-OHjcxF5q"
      },
      "source": [
        "code for bart rouge(score)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w4SWyMSJxJYo",
        "outputId": "de6f59b3-2e2a-4d95-bddc-aa3b3df0e11b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: rouge-score in /usr/local/lib/python3.10/dist-packages (0.1.2)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from rouge-score) (1.4.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from rouge-score) (3.8.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from rouge-score) (1.26.4)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from rouge-score) (1.16.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->rouge-score) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->rouge-score) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->rouge-score) (2024.5.15)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk->rouge-score) (4.66.5)\n",
            "Requirement already satisfied: pdfplumber in /usr/local/lib/python3.10/dist-packages (0.11.4)\n",
            "Requirement already satisfied: pdfminer.six==20231228 in /usr/local/lib/python3.10/dist-packages (from pdfplumber) (20231228)\n",
            "Requirement already satisfied: Pillow>=9.1 in /usr/local/lib/python3.10/dist-packages (from pdfplumber) (9.4.0)\n",
            "Requirement already satisfied: pypdfium2>=4.18.0 in /usr/local/lib/python3.10/dist-packages (from pdfplumber) (4.30.0)\n",
            "Requirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six==20231228->pdfplumber) (3.3.2)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six==20231228->pdfplumber) (43.0.0)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber) (1.17.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber) (2.22)\n",
            "\n",
            "Combined Short Summary:\n",
            " This research investigates the estimation of the presence of estrous cycle in Japanese dairy cows using Bayesian Network Model (BayES). It finds that the estimation accuracy of 270 Japanese anestrus Holstein dairy cows is almost 55% while using suspended likelihood, despite the limitations of previous methods. The study highlights the advantages of using BayES with other parameters, such as Body Condition Score and Parity Number, to enhance the accuracy of the estimation. Future research will focus on refining the model and This paper explores the role of estrous synchronization and other non-reproductive technologies in the cow and heifer reproduction process in the dairy industry. The estrous cycle of a cow or heifer is the period from one estrus (heat, phase of sexual activity) to the next, with a typical range of 18 to 24 days in length. It is a crucial factor in determining the progesterone level and the status of the heifer. The study evaluates various factors such as Thechronization Protocol, adopted from [17], addresses several key factors affecting cattle productivity, including poor body condition and increased weaning rates. It is a cost-effective management tool that measures the energy reserves of a cow by measuring the weightage of the cow with a fixed-scale range from 1 to 5 with 0.25 increments. This method is widely used in various animal management applications and can be divided into two groups: one for assessing weightage and the other for classifying This paper presents a Bayesian network model for identifying the presence or absence of anestrous cycle in bovine biovine studies. The proposed method achieves high accuracy in terms of time elapsed and Parity, achieving a mean error of 0.19% in relationships with, Postpartum interval, and number of parity. The study is well-organized as follows:\n",
            "\n",
            "1. **Bayesian Network Model**: The paper introduces a new model called the \"Bayesian This research proposes a quarter-point increase in fat pads in 280 individual cows from the Morinaga Dairy Service Co. Ltd. (), Japan and aims to improve the estimation of estrous cycle in dairy cattle. The study evaluates various fat pads, loin areas, pin and hook bones, and diagnostic tests conducted using the \n",
            "method (0.25 increase) and a simplified body scoring as a diagnostic test. The results indicate that thesemethods show good repeatability across various observers The paper explores the use of a Bayesian Network Model for evaluating estrous cycle prediction in the context of a fertility investigation conducted at a dairy farm in Iwate Prefecture, Japan. The study involved 280 individual Holstein cattle and assessed their Parity number, Parity ratio, and length of stay. The research found that the highest number of parity values were 9 and the lowest was 1, indicating that the cow with parity range of 1 to 4 might have a higher possibility to continue The dairy cow was a Japanese Holstein breed, which was inadvertently found anestrus due to some health hazards in its own farm. The Bayonet 6 software, developed by, Japan, was used to develop the proposed model. The study used 270 individual data and the rest of the dataset was processed using a geospatial-objective Bayesian network model.\n",
            "\n",
            "Bayesian network models are designed to visualize changes in posterior probability as the evidence increases, thereby improving the accuracy of The average estimation accuracy is 2.5 for 91-120 days (group 2) and Parity is 3, indicating that the absolute improbability of estrous cycle is same (80%). The Bayesian model is used to find out easily these relationships, leading to the discovery of many more relationships in the productivity of smallholder farmers.\n",
            "\n",
            "The paper is presented in the JICA International Journal of Advanced Computer Science and Applications, vol. 7, No. 11, 2016. The paper presents a more confident model for estimating estrous cycle with higher confidence than traditional methods. The proposed model, which includes both novice and experienced models, achieves an accuracy of 93% to 100%, making it a reliable tool for evaluating the presence of estrous cycles. The study highlights the potential of the proposed method in enhancing agricultural productivity and sustainability. Future research will focus on refining the model and exploring its application in other fields. This research introduces a novel method for determining the presence or absence of estrous cycle in cattle, using Bayesian network models. The method involves using body condition parameters, postpartum intervals, and parity in the model to enhance accuracy. The results and analysis reveal that the method significantly boosts the productivity of cattle, contributing to a new field of research in animal sciences. The research highlights the importance of accurate and optimum factors for cattle productivity in the bi-arena and as well as opens This study investigates the presence or absence of estrous cycles in cattle in Japan and other animal-intensivecountries. It proposes a novel method to identify the active cycle of a cow or heifer. The proposed method is evaluated using various machine learning techniques, including genetic algorithms, and genetic algorithms. The results show that the proposed method significantly improves accuracy and reliability compared to existing methods. Future research will focus on refining the model and exploring additional genetic algorithms to further improve its accuracy and applicability The study focuses on assessing the relationship between estrous cycle and pregnancy rate in cattle, focusing on the days after childbirth and or during the Postpartum Interval. The study finds that, while estrous synchronization protocol assists in maintaining fertility, it also contributes to raising the number of heifers and raising the overall population. The research highlights the importance of environmental factors in the evaluation of cattle, highlighting the need for further research to optimize the protocol and its applicability across different animal populations This study investigates the presence and absence of anestrus in Japanese Holstein cows using Bayesian network analysis, focusing on postpartum interval, interval, and parity. The study highlights the importance of using these factors to understand the presence or absence of estrous cycle and the optimal timing for artificial insemination to make them pregnant. The research highlights the potential of Bayesian networks to improve cattle management and herd management in Japan.\n",
            "\n",
            "The study involved surveying 280 different Holstein The paper presents a new Bayesian network tree for identifying estrous cycle in Cattle-intervestigation. The proposed method uses a two-tiered graph: the parent node indicates the cause, and the node shows the result. Each node provides valuable information, such as, ()/Days after childbirth, Parity number, and Estrous cycle. The details are explained in the following fig. 3.\n",
            "\n",
            "The paper introduces the proposed Bayesian Network tree structure to identify est This study provides a detailed view of fat pads on various cow's pelvic, loin areas, pin and hook bones, and skin areas. The study evaluates the fat pads of 280 individual cows from the dairy farm of Iwate-Iwate Prefecture, Japan under cooperation with the Japan Department of Agriculture.\n",
            "\n",
            "The results show that the following fat pads are significantly different from those observed in other studies:\n",
            "1. **No fat pads**: The skin area of each cow This research proposes a unique Bayesian Network Model to evaluate the status of dairy cows in their estrous cycle. The model was tested on a dairy farm in Iwate Prefecture, Japan, and observed with a unique number of cattle. The results showed that each of these 280 individual cattle had a unique Parity number and was observed in accordance with the proposed Bayesian network model. The study highlights the effectiveness of the proposed model in improving estrous cycles and suggests future research directions, including The paper presents a Bayesian network that combines the estimation of, and Parity, and testing data sets to achieve the best estimation accuracy for the presence of estrous cycle in cattle. The model is based on a Greedy search algorithm, and the 10 data sets used show that the proposed approach significantly improves the accuracy of the model and the 27\n",
            "individual sets used to validate andvalidate the proposed model. Additionally, when the proposed ensemble-based model reached higher confidence (less entropy The average estimation accuracy of the proposed model is 93% and the lowest is 50%, respectively. The average error for all data set is almost 55% and lowest is 49%. The log-likelihood is more than 0.7. Using the suspended rule on average likelihood, this research finds that, when the likelihood is <0.5, the highest estimation accuracy is almost 73%, with the lowest average being 50%.\n",
            "\n",
            "The research highlights the importance of using entropy to predict est This research evaluates the estimation accuracy of proposed with higher confidence using a Bayesian network model. The results show that the proposed model achieves high accuracy (0.15) and reliability (99% to 100%), making it the most reliable method for estimating estrous cycle presence in cattle. The research highlights the potential of the proposed method in enhancing agricultural productivity and productivity. Future research will focus on applying this approach to other types of cattle and optimizing resource use The research highlights the importance of objective estimation of estrous cycles in cattle productivity and suggests that more accurate and optimum factors for cattle productivity could be found. The paper concludes that the objective estimation definitely provides boost-up in the production of cattle.\n",
            "\n",
            "**Key Points:**\n",
            "\n",
            "1. **Estrous Cycle Variability**: The proposed model provides a more reliable and reliable method for determining estrous cycle status.\n",
            "2. **Methodology**: Experimental results show that the The paper explores the role of lactating cows in breast cancer research, focusing on the development of breast cancer screenings and the treatment of non-vegetable breast tissue. It introduces a novel approach that combines breast tissue screening with breast cancer screening, aiming to improve the identification and treatment of breast cancers. The study evaluates the effectiveness of this approach by comparing breast tissue screenings and breast tissue biopsy data. It finds that while breast tissue is highly sensitive to changes in blood type, breast tissue\n",
            "\n",
            "ROUGE Scores:\n",
            " {'rouge1': Score(precision=0.113595166163142, recall=0.8623853211009175, fmeasure=0.200747463961559), 'rouge2': Score(precision=0.06106408706166868, recall=0.46543778801843316, fmeasure=0.10796365579903795), 'rougeL': Score(precision=0.06525679758308157, recall=0.4954128440366973, fmeasure=0.11532301121195943)}\n"
          ]
        }
      ],
      "source": [
        "# Install necessary libraries\n",
        "!pip install rouge-score\n",
        "!pip install pdfplumber\n",
        "from transformers import BartTokenizer, BartForConditionalGeneration\n",
        "import pdfplumber\n",
        "import torch\n",
        "import re\n",
        "from rouge_score import rouge_scorer\n",
        "\n",
        "# Load the BART model and tokenizer\n",
        "model_path = \"/content/drive/MyDrive/new dataset/model_bart_v2\"  # Ensure the model path is correct\n",
        "tokenizer = BartTokenizer.from_pretrained(model_path)\n",
        "model = BartForConditionalGeneration.from_pretrained(model_path)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "# Function to extract text from multi-column PDF (same as before)\n",
        "def extract_text_from_columns(pdf_path):\n",
        "    text = \"\"\n",
        "    with pdfplumber.open(pdf_path) as pdf:\n",
        "        for page in pdf.pages:\n",
        "            left_bbox = (0, 0, page.width / 2, page.height)\n",
        "            right_bbox = (page.width / 2, 0, page.width, page.height)\n",
        "            left_text = page.within_bbox(left_bbox).extract_text()\n",
        "            right_text = page.within_bbox(right_bbox).extract_text()\n",
        "            combined_text = (left_text or '') + ' ' + (right_text or '')\n",
        "            text += combined_text + '\\n'\n",
        "    return text\n",
        "\n",
        "# Function to clean text (same as before)\n",
        "def clean_text(text):\n",
        "    cleaned_text = re.sub(r'\\b(?:[A-Z][A-Z0-9 ]+|Page \\d+|Header|Footer)\\b', '', text, flags=re.MULTILINE)\n",
        "    cleaned_text = re.sub(r'\\bReferences\\b.*$', '', cleaned_text, flags=re.S)\n",
        "    return cleaned_text\n",
        "\n",
        "# Function to extract specific sections from cleaned text (same as before)\n",
        "def extract_section(text, section_title):\n",
        "    pattern = rf'{section_title}[\\s\\S]*?(?=\\n[A-Z][A-Z\\s]+:|$)'\n",
        "    match = re.search(pattern, text, re.IGNORECASE)\n",
        "    if match:\n",
        "        return match.group().strip()\n",
        "    return ''\n",
        "\n",
        "# Function to chunk text and generate summaries\n",
        "def chunk_text(text, max_length=512):\n",
        "    tokens = tokenizer.encode(text, truncation=False)\n",
        "    chunks = [tokens[i:i + max_length] for i in range(0, len(tokens), max_length)]\n",
        "    return chunks\n",
        "\n",
        "# Function to generate short summaries with BART\n",
        "def generate_short_summary(text_chunk):\n",
        "    inputs = tokenizer(text_chunk, return_tensors=\"pt\", max_length=512, truncation=True, padding=\"longest\")\n",
        "    inputs = {key: value.to(device) for key, value in inputs.items()}\n",
        "    summary_ids = model.generate(\n",
        "        inputs[\"input_ids\"],\n",
        "        max_length=100,  # Adjust for shorter summaries\n",
        "        min_length=30,\n",
        "        length_penalty=2.0,\n",
        "        num_beams=4,\n",
        "        early_stopping=True\n",
        "    )\n",
        "    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "    return summary\n",
        "\n",
        "# Function to summarize large text and generate short summary (same as before)\n",
        "def summarize_large_text(text):\n",
        "    chunks = chunk_text(text)\n",
        "    short_summaries = [generate_short_summary(tokenizer.decode(chunk)) for chunk in chunks]\n",
        "    combined_short_summary = ' '.join(short_summaries)\n",
        "    return combined_short_summary\n",
        "\n",
        "# Function to calculate ROUGE score (same as before)\n",
        "def calculate_rouge(predicted_summary, reference_summary):\n",
        "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
        "    scores = scorer.score(reference_summary, predicted_summary)\n",
        "    return scores\n",
        "\n",
        "# Process PDF and generate summaries for specific sections (same as before)\n",
        "pdf_path = \"/content/drive/MyDrive/new dataset/mll.pdf\"\n",
        "document_text = extract_text_from_columns(pdf_path)\n",
        "cleaned_text = clean_text(document_text)\n",
        "\n",
        "# Extract sections (same as before)\n",
        "abstract_text = extract_section(cleaned_text, 'Abstract')\n",
        "results_text = extract_section(cleaned_text, 'Results')\n",
        "methodology_text = extract_section(cleaned_text, 'Methodology')\n",
        "conclusion_text = extract_section(cleaned_text, 'Conclusion')\n",
        "\n",
        "# Combine the extracted sections into a single text (same as before)\n",
        "combined_text = f\"{abstract_text}\\n{results_text}\\n{methodology_text}\\n{conclusion_text}\"\n",
        "\n",
        "# Generate combined short summary\n",
        "combined_short_summary = summarize_large_text(combined_text)\n",
        "\n",
        "# Print the combined short summary\n",
        "print(\"\\nCombined Short Summary:\\n\", combined_short_summary)\n",
        "\n",
        "# Reference summary for ROUGE score calculation\n",
        "reference_summary = \"\"\"\n",
        "   The paper titled \"Japanese Dairy Cattle Productivity Analysis using Bayesian Network Model (BNM)\" explores the application of a Bayesian Network Model to enhance the productivity of dairy cattle in Japan. The study involved 280 Japanese anestrus Holstein dairy cows and aimed to accurately estimate the presence of estrous cycles, achieving an overall accuracy of approximately 55% with the sample data. The model utilized key parameters such as Body Condition Score (BCS), Postpartum Interval (PPI), and parity to evaluate the estrous cycle's presence, revealing that BCS significantly influences other productivity factors.\n",
        "\n",
        "The research findings indicate that when BCS is at 2.5, with a PPI of 91-120 days and parity of 3, the probability of detecting an estrous cycle is 80%. The authors propose that the BNM can be further refined by incorporating additional parameters and validating the model with larger datasets, which would enhance its reliability and applicability in the livestock industry. The study emphasizes the importance of objective estimation methods over subjective ones, suggesting that the BNM could lead to improved productivity in the dairy sector not only in Japan but also in other countries.\n",
        "\n",
        "In conclusion, the research presents a novel approach to understanding and managing dairy cattle productivity through the use of Bayesian networks, highlighting the potential for future advancements in the field.\n",
        "\"\"\"\n",
        "\n",
        "# Calculate ROUGE scores for BART\n",
        "rouge_scores = calculate_rouge(combined_short_summary, reference_summary)\n",
        "\n",
        "# Print the ROUGE scores\n",
        "print(\"\\nROUGE Scores:\\n\", rouge_scores)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XqKn7dAe3BH6"
      },
      "source": [
        "**Making of hybrid model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PH9AUhSA3EUo",
        "outputId": "6ade68db-8d2d-4041-baf6-15cecab564b9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (0.33.0)\n",
            "Collecting accelerate\n",
            "  Downloading accelerate-0.34.2-py3-none-any.whl.metadata (19 kB)\n",
            "Collecting pdfplumber\n",
            "  Downloading pdfplumber-0.11.4-py3-none-any.whl.metadata (41 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m42.0/42.0 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: transformers[torch] in /usr/local/lib/python3.10/dist-packages (4.44.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (3.16.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.24.6)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (4.66.5)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.4.0+cu121)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Collecting pdfminer.six==20231228 (from pdfplumber)\n",
            "  Downloading pdfminer.six-20231228-py3-none-any.whl.metadata (4.2 kB)\n",
            "Requirement already satisfied: Pillow>=9.1 in /usr/local/lib/python3.10/dist-packages (from pdfplumber) (9.4.0)\n",
            "Collecting pypdfium2>=4.18.0 (from pdfplumber)\n",
            "  Downloading pypdfium2-4.30.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (48 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m48.5/48.5 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six==20231228->pdfplumber) (3.3.2)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six==20231228->pdfplumber) (43.0.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers[torch]) (2024.6.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers[torch]) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (1.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (3.1.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2024.8.30)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber) (1.17.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->transformers[torch]) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->transformers[torch]) (1.3.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber) (2.22)\n",
            "Downloading accelerate-0.34.2-py3-none-any.whl (324 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m324.4/324.4 kB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pdfplumber-0.11.4-py3-none-any.whl (59 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m59.2/59.2 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pdfminer.six-20231228-py3-none-any.whl (5.6 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m60.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pypdfium2-4.30.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.8 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m55.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pypdfium2, pdfminer.six, accelerate, pdfplumber\n",
            "  Attempting uninstall: accelerate\n",
            "    Found existing installation: accelerate 0.33.0\n",
            "    Uninstalling accelerate-0.33.0:\n",
            "      Successfully uninstalled accelerate-0.33.0\n",
            "Successfully installed accelerate-0.34.2 pdfminer.six-20231228 pdfplumber-0.11.4 pypdfium2-4.30.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (1085 > 512). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Combined Summary:\n",
            " This research paper presents a suite of deep learning-based regression models that yield a very high level of accuracy in stock price prediction. The research paper uses historical stock price data of a well-known company listed in the National Stock Exchange (NSE) of India during the period December 31, 2012 to January 9, 2015. The stock prices are recorded at five minutes time interval during each working day in each week. The proposed models are foural neural networks () and five long- and short The paper addresses the challenge of forecasting future stock prices and stock price movement patterns by proposing a multi-time series regression model based on the gamutal neural network () for predicting financial time series and stock price movements. It highlights the limitations of existing models, which often fail to achieve high accuracy in predicting stock prices. The proposed model is designed on the basis of the gamutal neural network () for predicting financial time series and stock price movements. The This paper presents a novel method for predicting the likelihood of a future event by using time series models. The first category use time series models like autoregressive integrated moving average (AMMA), vector autoregressive integrated moving average (VMMA), and autoregressive distributed lag (DDL). The second category use time series models like autoregressive integrated moving average (AMMA), vector autoregressive integrated moving average (VMMA), and autor\n"
          ]
        }
      ],
      "source": [
        "# Install necessary libraries\n",
        "!pip install transformers[torch] accelerate -U pdfplumber\n",
        "\n",
        "# Import libraries\n",
        "import pdfplumber\n",
        "from transformers import PegasusTokenizer, PegasusForConditionalGeneration, T5Tokenizer, T5ForConditionalGeneration\n",
        "import torch\n",
        "import re\n",
        "\n",
        "# Load Pegasus model and tokenizer\n",
        "pegasus_model_path = \"/content/drive/MyDrive/new dataset/model_v2\"\n",
        "pegasus_tokenizer = PegasusTokenizer.from_pretrained(pegasus_model_path)\n",
        "pegasus_model = PegasusForConditionalGeneration.from_pretrained(pegasus_model_path)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "pegasus_model.to(device)\n",
        "\n",
        "# Load T5 model and tokenizer\n",
        "t5_model_path = \"/content/drive/MyDrive/new dataset/model_t5_v2\"\n",
        "t5_tokenizer = T5Tokenizer.from_pretrained(t5_model_path)\n",
        "t5_model = T5ForConditionalGeneration.from_pretrained(t5_model_path)\n",
        "t5_model.to(device)\n",
        "\n",
        "# Function to extract text from multi-column PDF\n",
        "def extract_text_from_columns(pdf_path):\n",
        "    text = \"\"\n",
        "    with pdfplumber.open(pdf_path) as pdf:\n",
        "        for page in pdf.pages:\n",
        "            left_bbox = (0, 0, page.width / 2, page.height)\n",
        "            right_bbox = (page.width / 2, 0, page.width, page.height)\n",
        "\n",
        "            left_text = page.within_bbox(left_bbox).extract_text()\n",
        "            right_text = page.within_bbox(right_bbox).extract_text()\n",
        "\n",
        "            combined_text = (left_text or '') + ' ' + (right_text or '')\n",
        "            text += combined_text + '\\n'\n",
        "    return text\n",
        "\n",
        "# Function to clean text by removing headers, footers, and references\n",
        "def clean_text(text):\n",
        "    cleaned_text = re.sub(r'\\b(?:[A-Z][A-Z0-9 ]+|Page \\d+|Header|Footer)\\b', '', text, flags=re.MULTILINE)\n",
        "    cleaned_text = re.sub(r'\\bReferences\\b.*$', '', cleaned_text, flags=re.S)\n",
        "    return cleaned_text\n",
        "\n",
        "# Function to extract sections from the cleaned text\n",
        "def extract_section(text, section_title):\n",
        "    # Regex to find sections based on title and extract the following text until the next section\n",
        "    pattern = rf'{section_title}[\\s\\S]*?(?=\\n[A-Z][A-Z\\s]+:|$)'\n",
        "    match = re.search(pattern, text, re.IGNORECASE)\n",
        "    if match:\n",
        "        return match.group().strip()\n",
        "    return ''\n",
        "\n",
        "# Function to chunk text\n",
        "def chunk_text(text, max_length=512):\n",
        "    tokens = pegasus_tokenizer.encode(text, truncation=False)\n",
        "    chunks = [tokens[i:i + max_length] for i in range(0, len(tokens), max_length)]\n",
        "    return chunks\n",
        "\n",
        "# Function to generate summaries using Pegasus\n",
        "def generate_summary_pegasus(text_chunk, max_length=100):\n",
        "    inputs = pegasus_tokenizer(text_chunk, return_tensors=\"pt\", max_length=512, truncation=True, padding=\"longest\")\n",
        "    inputs = {key: value.to(device) for key, value in inputs.items()}\n",
        "    summary_ids = pegasus_model.generate(\n",
        "        inputs[\"input_ids\"],\n",
        "        max_length=max_length,\n",
        "        min_length=30,\n",
        "        length_penalty=2.0,\n",
        "        num_beams=4,\n",
        "        early_stopping=True\n",
        "    )\n",
        "    summary = pegasus_tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "    return summary\n",
        "\n",
        "# Function to refine summaries using T5\n",
        "def refine_summary_t5(pegasus_summary, max_length=100):\n",
        "    inputs = t5_tokenizer(pegasus_summary, return_tensors=\"pt\", max_length=512, truncation=True, padding=\"longest\")\n",
        "    inputs = {key: value.to(device) for key, value in inputs.items()}\n",
        "    summary_ids = t5_model.generate(\n",
        "        inputs[\"input_ids\"],\n",
        "        max_length=max_length,\n",
        "        min_length=30,\n",
        "        length_penalty=2.0,\n",
        "        num_beams=4,\n",
        "        early_stopping=True\n",
        "    )\n",
        "    refined_summary = t5_tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "    return refined_summary\n",
        "\n",
        "# Function to summarize large text\n",
        "def summarize_large_text(text):\n",
        "    chunks = chunk_text(text)\n",
        "    pegasus_summaries = [generate_summary_pegasus(pegasus_tokenizer.decode(chunk)) for chunk in chunks]\n",
        "    refined_summaries = [refine_summary_t5(summary) for summary in pegasus_summaries]\n",
        "    combined_summary = ' '.join(refined_summaries)\n",
        "    return combined_summary\n",
        "\n",
        "# Process PDF and generate summaries\n",
        "pdf_path = \"/content/drive/MyDrive/Colab Notebooks/test5.pdf\"\n",
        "document_text = extract_text_from_columns(pdf_path)\n",
        "cleaned_text = clean_text(document_text)\n",
        "\n",
        "# Extract sections\n",
        "abstract_text = extract_section(cleaned_text, 'Abstract')\n",
        "results_text = extract_section(cleaned_text, 'Results')\n",
        "methodology_text = extract_section(cleaned_text, 'Methodology')\n",
        "conclusion_text = extract_section(cleaned_text, 'Conclusion')\n",
        "introduction_text = extract_section(cleaned_text, 'Introduction')\n",
        "\n",
        "# Combine the extracted sections into a single text\n",
        "combined_text = f\"{introduction_text}\\n{abstract_text}\\n{results_text}\\n{methodology_text}\\n{conclusion_text}\"\n",
        "\n",
        "# Generate combined summary from both models\n",
        "combined_summary = summarize_large_text(combined_text)\n",
        "\n",
        "# Print the combined summary\n",
        "print(\"\\nCombined Summary:\\n\", combined_summary)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VL3LiBj_mc0s"
      },
      "source": [
        "making the summary proepr without any repeated sentences and ending properly\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ipfYrxWbmh8c",
        "outputId": "f9b811b4-4cbf-4458-e1a8-69ed6276d240"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (0.34.2)\n",
            "Requirement already satisfied: pdfplumber in /usr/local/lib/python3.10/dist-packages (0.11.4)\n",
            "Requirement already satisfied: transformers[torch] in /usr/local/lib/python3.10/dist-packages (4.44.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (3.16.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.24.6)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (4.66.5)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.4.0+cu121)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pdfminer.six==20231228 in /usr/local/lib/python3.10/dist-packages (from pdfplumber) (20231228)\n",
            "Requirement already satisfied: Pillow>=9.1 in /usr/local/lib/python3.10/dist-packages (from pdfplumber) (9.4.0)\n",
            "Requirement already satisfied: pypdfium2>=4.18.0 in /usr/local/lib/python3.10/dist-packages (from pdfplumber) (4.30.0)\n",
            "Requirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six==20231228->pdfplumber) (3.3.2)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six==20231228->pdfplumber) (43.0.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers[torch]) (2024.6.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers[torch]) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (1.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (3.1.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2024.8.30)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber) (1.17.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->transformers[torch]) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->transformers[torch]) (1.3.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber) (2.22)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (1085 > 512). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Cleaned Combined Summary:\n",
            " This research paper presents a suite of deep learning-based regression models that yield a very high level of accuracy in stock price prediction. The research paper uses historical stock price data of a well-known company listed in the National Stock Exchange (NSE) of India during the period December 31, 2012 to January 9, 2015. The stock prices are recorded at five minutes time interval during each working day in each week. The proposed models are foural neural networks () and five long- and short The paper addresses the challenge of forecasting future stock prices and stock price movement patterns by proposing a multi-time series regression model based on the gamutal neural network () for predicting financial time series and stock price movements. It highlights the limitations of existing models, which often fail to achieve high accuracy in predicting stock prices. The proposed model is designed on the basis of the gamutal neural network () for predicting financial time series and stock price movements. The This paper presents a novel method for predicting the likelihood of a future event by using time series models. The first category use time series models like autoregressive integrated moving average (AMMA), vector autoregressive integrated moving average (VMMA), and autoregressive distributed lag (DDL). The second category use time series models like autoregressive integrated moving average (AMMA), vector autoregressive integrated moving average (VMMA), and autor.\n"
          ]
        }
      ],
      "source": [
        "# Install necessary libraries\n",
        "!pip install transformers[torch] accelerate -U pdfplumber\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Import libraries\n",
        "import pdfplumber\n",
        "from transformers import PegasusTokenizer, PegasusForConditionalGeneration, T5Tokenizer, T5ForConditionalGeneration\n",
        "import torch\n",
        "import re\n",
        "\n",
        "# Load Pegasus model and tokenizer\n",
        "pegasus_model_path = \"/content/drive/MyDrive/new dataset/model_v2\"\n",
        "pegasus_tokenizer = PegasusTokenizer.from_pretrained(pegasus_model_path)\n",
        "pegasus_model = PegasusForConditionalGeneration.from_pretrained(pegasus_model_path)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "pegasus_model.to(device)\n",
        "\n",
        "# Load T5 model and tokenizer\n",
        "t5_model_path = \"/content/drive/MyDrive/new dataset/model_t5_v2\"\n",
        "t5_tokenizer = T5Tokenizer.from_pretrained(t5_model_path)\n",
        "t5_model = T5ForConditionalGeneration.from_pretrained(t5_model_path)\n",
        "t5_model.to(device)\n",
        "\n",
        "# Function to extract text from multi-column PDF\n",
        "def extract_text_from_columns(pdf_path):\n",
        "    text = \"\"\n",
        "    with pdfplumber.open(pdf_path) as pdf:\n",
        "        for page in pdf.pages:\n",
        "            left_bbox = (0, 0, page.width / 2, page.height)\n",
        "            right_bbox = (page.width / 2, 0, page.width, page.height)\n",
        "\n",
        "            left_text = page.within_bbox(left_bbox).extract_text()\n",
        "            right_text = page.within_bbox(right_bbox).extract_text()\n",
        "\n",
        "            combined_text = (left_text or '') + ' ' + (right_text or '')\n",
        "            text += combined_text + '\\n'\n",
        "    return text\n",
        "\n",
        "# Function to clean text by removing headers, footers, and references\n",
        "def clean_text(text):\n",
        "    cleaned_text = re.sub(r'\\b(?:[A-Z][A-Z0-9 ]+|Page \\d+|Header|Footer)\\b', '', text, flags=re.MULTILINE)\n",
        "    cleaned_text = re.sub(r'\\bReferences\\b.*$', '', cleaned_text, flags=re.S)\n",
        "    return cleaned_text\n",
        "\n",
        "# Function to extract sections from the cleaned text\n",
        "def extract_section(text, section_title):\n",
        "    # Regex to find sections based on title and extract the following text until the next section\n",
        "    pattern = rf'{section_title}[\\s\\S]*?(?=\\n[A-Z][A-Z\\s]+:|$)'\n",
        "    match = re.search(pattern, text, re.IGNORECASE)\n",
        "    if match:\n",
        "        return match.group().strip()\n",
        "    return ''\n",
        "\n",
        "# Function to chunk text\n",
        "def chunk_text(text, max_length=512):\n",
        "    tokens = pegasus_tokenizer.encode(text, truncation=False)\n",
        "    chunks = [tokens[i:i + max_length] for i in range(0, len(tokens), max_length)]\n",
        "    return chunks\n",
        "\n",
        "# Function to generate summaries using Pegasus\n",
        "def generate_summary_pegasus(text_chunk, max_length=100):\n",
        "    inputs = pegasus_tokenizer(text_chunk, return_tensors=\"pt\", max_length=512, truncation=True, padding=\"longest\")\n",
        "    inputs = {key: value.to(device) for key, value in inputs.items()}\n",
        "    summary_ids = pegasus_model.generate(\n",
        "        inputs[\"input_ids\"],\n",
        "        max_length=max_length,\n",
        "        min_length=30,\n",
        "        length_penalty=2.0,\n",
        "        num_beams=4,\n",
        "        early_stopping=True\n",
        "    )\n",
        "    summary = pegasus_tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "    return summary\n",
        "\n",
        "# Function to refine summaries using T5\n",
        "def refine_summary_t5(pegasus_summary, max_length=100):\n",
        "    inputs = t5_tokenizer(pegasus_summary, return_tensors=\"pt\", max_length=512, truncation=True, padding=\"longest\")\n",
        "    inputs = {key: value.to(device) for key, value in inputs.items()}\n",
        "    summary_ids = t5_model.generate(\n",
        "        inputs[\"input_ids\"],\n",
        "        max_length=max_length,\n",
        "        min_length=30,\n",
        "        length_penalty=2.0,\n",
        "        num_beams=4,\n",
        "        early_stopping=True\n",
        "    )\n",
        "    refined_summary = t5_tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "    return refined_summary\n",
        "\n",
        "# Function to clean and ensure proper sentence boundaries\n",
        "def clean_summary(summary):\n",
        "    # Tokenize the summary into sentences\n",
        "    sentences = sent_tokenize(summary)\n",
        "\n",
        "    # Remove duplicate sentences (but allow one repetition)\n",
        "    unique_sentences = []\n",
        "    sentence_count = {}\n",
        "\n",
        "    for sentence in sentences:\n",
        "        # Clean sentence from any unnecessary spaces or fragments\n",
        "        sentence = sentence.strip()\n",
        "        if len(sentence) > 0 and (sentence[-1] not in '.!?'):\n",
        "            sentence += '.'\n",
        "\n",
        "        # Count sentence occurrences and keep it at most twice\n",
        "        if sentence in sentence_count:\n",
        "            sentence_count[sentence] += 1\n",
        "        else:\n",
        "            sentence_count[sentence] = 1\n",
        "\n",
        "        # Add sentence if it appears once or twice\n",
        "        if sentence_count[sentence] <= 2:\n",
        "            unique_sentences.append(sentence)\n",
        "\n",
        "    # Join the unique sentences back into a cleaned summary\n",
        "    cleaned_summary = ' '.join(unique_sentences)\n",
        "    return cleaned_summary\n",
        "\n",
        "# Function to summarize large text\n",
        "def summarize_large_text(text):\n",
        "    chunks = chunk_text(text)\n",
        "    pegasus_summaries = [generate_summary_pegasus(pegasus_tokenizer.decode(chunk)) for chunk in chunks]\n",
        "    refined_summaries = [refine_summary_t5(summary) for summary in pegasus_summaries]\n",
        "\n",
        "    # Combine all refined summaries into one text\n",
        "    combined_summary = ' '.join(refined_summaries)\n",
        "\n",
        "    # Clean and ensure proper sentence boundaries and remove extra repetitions\n",
        "    final_summary = clean_summary(combined_summary)\n",
        "\n",
        "    return final_summary\n",
        "\n",
        "# Process PDF and generate summaries\n",
        "pdf_path = \"/content/drive/MyDrive/Colab Notebooks/test5.pdf\"\n",
        "document_text = extract_text_from_columns(pdf_path)\n",
        "cleaned_text = clean_text(document_text)\n",
        "\n",
        "# Extract sections\n",
        "abstract_text = extract_section(cleaned_text, 'Abstract')\n",
        "results_text = extract_section(cleaned_text, 'Results')\n",
        "methodology_text = extract_section(cleaned_text, 'Methodology')\n",
        "conclusion_text = extract_section(cleaned_text, 'Conclusion')\n",
        "introduction_text = extract_section(cleaned_text, 'Introduction')\n",
        "\n",
        "# Combine the extracted sections into a single text\n",
        "combined_text = f\"{introduction_text}\\n{abstract_text}\\n{results_text}\\n{methodology_text}\\n{conclusion_text}\"\n",
        "\n",
        "# Generate combined summary from both models\n",
        "combined_summary = summarize_large_text(combined_text)\n",
        "\n",
        "# Print the cleaned combined summary\n",
        "print(\"\\nCleaned Combined Summary:\\n\", combined_summary)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zmev83-N0_e_"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hf85p3pY0_wm",
        "outputId": "2c70f706-515e-4e81-b4e5-ff89fa8fa6a8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: flask-ngrok in /usr/local/lib/python3.10/dist-packages (0.0.25)\n",
            "Requirement already satisfied: Flask>=0.8 in /usr/local/lib/python3.10/dist-packages (from flask-ngrok) (2.2.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from flask-ngrok) (2.32.3)\n",
            "Requirement already satisfied: Werkzeug>=2.2.2 in /usr/local/lib/python3.10/dist-packages (from Flask>=0.8->flask-ngrok) (3.0.4)\n",
            "Requirement already satisfied: Jinja2>=3.0 in /usr/local/lib/python3.10/dist-packages (from Flask>=0.8->flask-ngrok) (3.1.4)\n",
            "Requirement already satisfied: itsdangerous>=2.0 in /usr/local/lib/python3.10/dist-packages (from Flask>=0.8->flask-ngrok) (2.2.0)\n",
            "Requirement already satisfied: click>=8.0 in /usr/local/lib/python3.10/dist-packages (from Flask>=0.8->flask-ngrok) (8.1.7)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->flask-ngrok) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->flask-ngrok) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->flask-ngrok) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->flask-ngrok) (2024.8.30)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from Jinja2>=3.0->Flask>=0.8->flask-ngrok) (2.1.5)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.4.0+cu121)\n",
            "Requirement already satisfied: pdfplumber in /usr/local/lib/python3.10/dist-packages (0.11.4)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.24.7)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.6.1)\n",
            "Requirement already satisfied: pdfminer.six==20231228 in /usr/local/lib/python3.10/dist-packages (from pdfplumber) (20231228)\n",
            "Requirement already satisfied: Pillow>=9.1 in /usr/local/lib/python3.10/dist-packages (from pdfplumber) (9.4.0)\n",
            "Requirement already satisfied: pypdfium2>=4.18.0 in /usr/local/lib/python3.10/dist-packages (from pdfplumber) (4.30.0)\n",
            "Requirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six==20231228->pdfplumber) (3.3.2)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six==20231228->pdfplumber) (43.0.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber) (1.17.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber) (2.22)\n"
          ]
        }
      ],
      "source": [
        "!pip install flask-ngrok\n",
        "!pip install transformers torch pdfplumber nltk\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "J1dC4Nin1Qrf",
        "outputId": "5f6c285d-fcb3-4a70-8ef9-10c6c89a5827"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (0.34.2)\n",
            "Requirement already satisfied: pdfplumber in /usr/local/lib/python3.10/dist-packages (0.11.4)\n",
            "Collecting rouge_score\n",
            "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: transformers[torch] in /usr/local/lib/python3.10/dist-packages (4.44.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (3.16.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.24.7)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (4.66.5)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.4.0+cu121)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pdfminer.six==20231228 in /usr/local/lib/python3.10/dist-packages (from pdfplumber) (20231228)\n",
            "Requirement already satisfied: Pillow>=9.1 in /usr/local/lib/python3.10/dist-packages (from pdfplumber) (9.4.0)\n",
            "Requirement already satisfied: pypdfium2>=4.18.0 in /usr/local/lib/python3.10/dist-packages (from pdfplumber) (4.30.0)\n",
            "Requirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six==20231228->pdfplumber) (3.3.2)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six==20231228->pdfplumber) (43.0.1)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.4.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from rouge_score) (3.8.1)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.16.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers[torch]) (2024.6.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers[torch]) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (1.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (3.1.4)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (1.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2024.8.30)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber) (1.17.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->transformers[torch]) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->transformers[torch]) (1.3.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber) (2.22)\n",
            "Building wheels for collected packages: rouge_score\n",
            "  Building wheel for rouge_score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24935 sha256=47a63b2e383a1fdb42f28d0ecc84bf1db9f3881851799795b02d0a88b0fad5aa\n",
            "  Stored in directory: /root/.cache/pip/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\n",
            "Successfully built rouge_score\n",
            "Installing collected packages: rouge_score\n",
            "Successfully installed rouge_score-0.1.2\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-e076c7968f87>\u001b[0m in \u001b[0;36m<cell line: 18>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mpegasus_model_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/content/drive/MyDrive/new dataset/model_v2\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mpegasus_tokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPegasusTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpegasus_model_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mpegasus_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPegasusForConditionalGeneration\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpegasus_model_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cuda\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mpegasus_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3830\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mContextManagers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minit_contexts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3831\u001b[0m             \u001b[0;31m# Let's make sure we don't run the init function of buffer modules\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3832\u001b[0;31m             \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mmodel_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3833\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3834\u001b[0m         \u001b[0;31m# make sure we use the model's config since the __init__ call might have copied it\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/pegasus/modeling_pegasus.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m   1252\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mPegasusConfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1253\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1254\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPegasusModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1255\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister_buffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"final_logits_bias\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshared\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_embeddings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlm_head\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0md_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshared\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_embeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/pegasus/modeling_pegasus.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m   1103\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshared\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEmbedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0md_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1105\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPegasusEncoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshared\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1106\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPegasusDecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshared\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/pegasus/modeling_pegasus.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, config, embed_tokens)\u001b[0m\n\u001b[1;32m    632\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEmbedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membed_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    633\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 634\u001b[0;31m         self.embed_positions = PegasusSinusoidalPositionalEmbedding(\n\u001b[0m\u001b[1;32m    635\u001b[0m             \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_position_embeddings\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    636\u001b[0m             \u001b[0membed_dim\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/pegasus/modeling_pegasus.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, num_positions, embedding_dim, padding_idx)\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_positions\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding_dim\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_positions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_weight\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/pegasus/modeling_pegasus.py\u001b[0m in \u001b[0;36m_init_weight\u001b[0;34m(out)\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0mn_pos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m         position_enc = np.array(\n\u001b[0;32m---> 86\u001b[0;31m             \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpos\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mj\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mpos\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_pos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m         )\n\u001b[1;32m     88\u001b[0m         \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m  \u001b[0;31m# set early to avoid an error in pytorch-1.8+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/pegasus/modeling_pegasus.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0mn_pos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m         position_enc = np.array(\n\u001b[0;32m---> 86\u001b[0;31m             \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpos\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mj\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mpos\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_pos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m         )\n\u001b[1;32m     88\u001b[0m         \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m  \u001b[0;31m# set early to avoid an error in pytorch-1.8+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/pegasus/modeling_pegasus.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0mn_pos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m         position_enc = np.array(\n\u001b[0;32m---> 86\u001b[0;31m             \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpos\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mj\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mpos\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_pos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m         )\n\u001b[1;32m     88\u001b[0m         \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m  \u001b[0;31m# set early to avoid an error in pytorch-1.8+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# Install necessary libraries\n",
        "!pip install transformers[torch] accelerate -U pdfplumber rouge_score\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Import libraries\n",
        "import pdfplumber\n",
        "from transformers import PegasusTokenizer, PegasusForConditionalGeneration, T5Tokenizer, T5ForConditionalGeneration\n",
        "import torch\n",
        "import re\n",
        "from google.colab import files  # For uploading files in Google Colab\n",
        "from rouge_score import rouge_scorer\n",
        "\n",
        "# Load Pegasus model and tokenizer\n",
        "pegasus_model_path = \"/content/drive/MyDrive/new dataset/model_v2\"\n",
        "pegasus_tokenizer = PegasusTokenizer.from_pretrained(pegasus_model_path)\n",
        "pegasus_model = PegasusForConditionalGeneration.from_pretrained(pegasus_model_path)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "pegasus_model.to(device)\n",
        "\n",
        "# Load T5 model and tokenizer\n",
        "t5_model_path = \"/content/drive/MyDrive/new dataset/model_t5_v2\"\n",
        "t5_tokenizer = T5Tokenizer.from_pretrained(t5_model_path)\n",
        "t5_model = T5ForConditionalGeneration.from_pretrained(t5_model_path)\n",
        "t5_model.to(device)\n",
        "\n",
        "# Define the reference summary directly in the code\n",
        "reference_summary = \"\"\"\n",
        "Your reference summary text goes here.\n",
        "This summary will be used to calculate the ROUGE score against the generated summary.\n",
        "Ensure this text accurately represents the type of summaries you expect the model to generate.\n",
        "\"\"\"\n",
        "\n",
        "# Function to extract text from multi-column PDF\n",
        "def extract_text_from_columns(pdf_path):\n",
        "    text = \"\"\n",
        "    with pdfplumber.open(pdf_path) as pdf:\n",
        "        for page in pdf.pages:\n",
        "            left_bbox = (0, 0, page.width / 2, page.height)\n",
        "            right_bbox = (page.width / 2, 0, page.width, page.height)\n",
        "\n",
        "            left_text = page.within_bbox(left_bbox).extract_text()\n",
        "            right_text = page.within_bbox(right_bbox).extract_text()\n",
        "\n",
        "            combined_text = (left_text or '') + ' ' + (right_text or '')\n",
        "            text += combined_text + '\\n'\n",
        "    return text\n",
        "\n",
        "# Function to clean text by removing headers, footers, and references\n",
        "def clean_text(text):\n",
        "    cleaned_text = re.sub(r'\\b(?:[A-Z][A-Z0-9 ]+|Page \\d+|Header|Footer)\\b', '', text, flags=re.MULTILINE)\n",
        "    cleaned_text = re.sub(r'\\bReferences\\b.*$', '', cleaned_text, flags=re.S)\n",
        "    return cleaned_text\n",
        "\n",
        "# Function to extract sections from the cleaned text\n",
        "def extract_section(text, section_title):\n",
        "    pattern = rf'{section_title}[\\s\\S]*?(?=\\n[A-Z][A-Z\\s]+:|$)'\n",
        "    match = re.search(pattern, text, re.IGNORECASE)\n",
        "    if match:\n",
        "        return match.group().strip()\n",
        "    return ''\n",
        "\n",
        "# Function to chunk text\n",
        "def chunk_text(text, max_length=512):\n",
        "    tokens = pegasus_tokenizer.encode(text, truncation=False)\n",
        "    chunks = [tokens[i:i + max_length] for i in range(0, len(tokens), max_length)]\n",
        "    return chunks\n",
        "\n",
        "# Function to generate summaries using Pegasus\n",
        "def generate_summary_pegasus(text_chunk, max_length=100):\n",
        "    inputs = pegasus_tokenizer(text_chunk, return_tensors=\"pt\", max_length=512, truncation=True, padding=\"longest\")\n",
        "    inputs = {key: value.to(device) for key, value in inputs.items()}\n",
        "    summary_ids = pegasus_model.generate(\n",
        "        inputs[\"input_ids\"],\n",
        "        max_length=max_length,\n",
        "        min_length=30,\n",
        "        length_penalty=2.0,\n",
        "        num_beams=4,\n",
        "        early_stopping=True\n",
        "    )\n",
        "    summary = pegasus_tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "    return summary\n",
        "\n",
        "# Function to refine summaries using T5\n",
        "def refine_summary_t5(pegasus_summary, max_length=100):\n",
        "    inputs = t5_tokenizer(pegasus_summary, return_tensors=\"pt\", max_length=512, truncation=True, padding=\"longest\")\n",
        "    inputs = {key: value.to(device) for key, value in inputs.items()}\n",
        "    summary_ids = t5_model.generate(\n",
        "        inputs[\"input_ids\"],\n",
        "        max_length=max_length,\n",
        "        min_length=30,\n",
        "        length_penalty=2.0,\n",
        "        num_beams=4,\n",
        "        early_stopping=True\n",
        "    )\n",
        "    refined_summary = t5_tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "    return refined_summary\n",
        "\n",
        "# Function to clean and ensure proper sentence boundaries\n",
        "def clean_summary(summary):\n",
        "    sentences = sent_tokenize(summary)\n",
        "\n",
        "    unique_sentences = []\n",
        "    sentence_count = {}\n",
        "\n",
        "    for sentence in sentences:\n",
        "        sentence = sentence.strip()\n",
        "        if len(sentence) > 0 and (sentence[-1] not in '.!?'):\n",
        "            sentence += '.'\n",
        "\n",
        "        if sentence in sentence_count:\n",
        "            sentence_count[sentence] += 1\n",
        "        else:\n",
        "            sentence_count[sentence] = 1\n",
        "\n",
        "        if sentence_count[sentence] <= 2:\n",
        "            unique_sentences.append(sentence)\n",
        "\n",
        "    cleaned_summary = ' '.join(unique_sentences)\n",
        "    return cleaned_summary\n",
        "\n",
        "# Function to summarize large text\n",
        "def summarize_large_text(text):\n",
        "    chunks = chunk_text(text)\n",
        "    pegasus_summaries = [generate_summary_pegasus(pegasus_tokenizer.decode(chunk)) for chunk in chunks]\n",
        "    refined_summaries = [refine_summary_t5(summary) for summary in pegasus_summaries]\n",
        "\n",
        "    combined_summary = ' '.join(refined_summaries)\n",
        "    final_summary = clean_summary(combined_summary)\n",
        "\n",
        "    return final_summary\n",
        "\n",
        "# Function to calculate ROUGE score\n",
        "def calculate_rouge(reference, generated):\n",
        "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
        "    scores = scorer.score(reference, generated)\n",
        "    return scores\n",
        "\n",
        "# Main function to process PDF and summarize\n",
        "def main():\n",
        "    print(\"Welcome to the PDF Summarizer!\")\n",
        "    uploaded = files.upload()  # User uploads the PDF file\n",
        "\n",
        "    for pdf_name in uploaded.keys():\n",
        "        print(f\"Processing file: {pdf_name}\")\n",
        "        document_text = extract_text_from_columns(pdf_name)\n",
        "        cleaned_text = clean_text(document_text)\n",
        "\n",
        "        print(\"Extracting relevant sections...\")\n",
        "        abstract_text = extract_section(cleaned_text, 'Abstract')\n",
        "        results_text = extract_section(cleaned_text, 'Results')\n",
        "        methodology_text = extract_section(cleaned_text, 'Methodology')\n",
        "        conclusion_text = extract_section(cleaned_text, 'Conclusion')\n",
        "        introduction_text = extract_section(cleaned_text, 'Introduction')\n",
        "\n",
        "        combined_text = f\"{introduction_text}\\n{abstract_text}\\n{results_text}\\n{methodology_text}\\n{conclusion_text}\"\n",
        "\n",
        "        print(\"Generating summary...\")\n",
        "        combined_summary = summarize_large_text(combined_text)\n",
        "\n",
        "        print(\"\\nFinal Summary:\\n\", combined_summary)\n",
        "\n",
        "        # Calculate ROUGE scores\n",
        "        rouge_scores = calculate_rouge(reference_summary, combined_summary)\n",
        "\n",
        "        print(\"\\nROUGE Scores:\\n\", rouge_scores)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y1lH496S_UO5"
      },
      "source": [
        "making rouge score maximum"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "U1HW-s0Z_WbH",
        "outputId": "cf446244-6cd7-4302-a2d0-b744b637bfc9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (0.34.2)\n",
            "Requirement already satisfied: pdfplumber in /usr/local/lib/python3.10/dist-packages (0.11.4)\n",
            "Requirement already satisfied: rouge_score in /usr/local/lib/python3.10/dist-packages (0.1.2)\n",
            "Requirement already satisfied: transformers[torch] in /usr/local/lib/python3.10/dist-packages (4.44.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (3.16.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.24.7)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (4.66.5)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.4.0+cu121)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pdfminer.six==20231228 in /usr/local/lib/python3.10/dist-packages (from pdfplumber) (20231228)\n",
            "Requirement already satisfied: Pillow>=9.1 in /usr/local/lib/python3.10/dist-packages (from pdfplumber) (9.4.0)\n",
            "Requirement already satisfied: pypdfium2>=4.18.0 in /usr/local/lib/python3.10/dist-packages (from pdfplumber) (4.30.0)\n",
            "Requirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six==20231228->pdfplumber) (3.3.2)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six==20231228->pdfplumber) (43.0.1)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.4.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from rouge_score) (3.8.1)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.16.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers[torch]) (2024.6.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers[torch]) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (1.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (3.1.4)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (1.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2024.8.30)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber) (1.17.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->transformers[torch]) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->transformers[torch]) (1.3.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber) (2.22)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Welcome to the PDF Summarizer!\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-b77768bc-5762-489b-b313-a2a71fd2ac53\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-b77768bc-5762-489b-b313-a2a71fd2ac53\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving CONIT2022Paper0627.pdf to CONIT2022Paper0627.pdf\n",
            "Processing file: CONIT2022Paper0627.pdf\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (8821 > 512). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting relevant sections...\n",
            "Generating summary...\n",
            "\n",
            "Final Summary:\n",
            " This paper presents a machine learning model designed to identify and classify user profiles as genuine or not genuine on online social networks. It aims to address the challenge of detecting social media users from cyber criminals by developing a machine learning model that identifies and classifies user profiles as genuine or not genuine. The proposed model achieved an average accuracy of 94% in the classification task considering all three datasets. The study highlights the importance of identifying genuine user profiles on social media The paper addresses the issue of fake profiles on social networking sites like Facebook, Twitter, LinkedIn, Orkut, MySpace. It highlights the growing problem of fake profiles and the need for an automated system to detect them. The paper proposes a novel algorithm based on machine learning (ML) to detect fake accounts using a combination of text and image recognition techniques. The algorithm is designed to detect the presence of fake accounts using a combination of text and image recognition techniques. The The paper addresses the issue of fake user profiles in online social media networks, which can be used by attackers to spread rumours and spam. It proposes a novel method for classifying genuine and fake user profiles using Random forest and Neural Network. The study evaluates three datasets: Facebook, Instagram, and Twitter. It finds that Random forest has the highest accuracy of 97.6% in identifying genuine user profiles and 97.6% in identifying fake users The paper addresses the challenge of identifying fake users on social networks like Facebook, Twitter, Instagram, Snapchat, and Vine. It proposes a framework based on Random Forest to identify fake users using a dataset of 922 profiles. The framework has an accuracy of 95% and can be extended to handle different features like education, work, gender, no. of friends, and engagement rate. The dataset is given by Barracuda Labs and can be used for This paper presents a novel method for detecting fake or cloned Twitter accounts using supervised machine learning. The method was developed using a dataset consisting of 1002 genuine and 201 fake accounts. The dataset was purchased from fastfollowerz.com, intertwitter.com, and twittertechnology.com. The results showed that the proposed method achieved an accuracy of ninety-nine percent. The paper suggests that future work should focus on improving The paper introduces a new classification model called Genuine User Profile Classification (GVC) that classifies user profiles as genuine or not. The model uses the dataset of Facebook, Instagram and Twitter users to determine whether the user is genuine or not. The model is tested on three different datasets: real users, spam users, and spam users. The results show that Genuine User Profile Classification (GVC) is the most accurate and performs best on all three datasets. The This paper presents a novel method for classifying data into fake and genuine profiles using Neural Network and Random Forest algorithms. The method involves training and testing different classifiers in three datasets: Facebook, Instagram and Twitter. In the training phase, the highest performance classifier is used to predict whether the profile is genuine or not as depicted in Fig 2. In the testing phase, the highest performance classifier is used to predict whether the profile is fake or not as depicte This paper presents a novel method for classifying social media users based on their characteristics. The method involves selecting three sets of attributes from the Instagram dataset: 'u'listed_count', 'u'sex_code', 'u'lang_code', 'u'statuses_count', and 'u'friends_count'. The classifiers used are Random Forest, Neural Network, and Network. The This paper presents a classification model for identifying genuine users on online social networks such as Instagram, Facebook, and Twitter. The model is based on the Precision -score Accuracy Recall Random Forest algorithm, which achieves the highest accuracy of 95%. The paper highlights that Random Forest performs better than Neural Network on Instagram, Facebook, and Twitter datasets, with an average accuracy of 95%. The model can be tested against performance measures other than accuracy and Social networking sites are platforms that people use to make friends, build social communities, and connect virtually with people who share similar interests. They are platforms that people use to make friends, build social communities, and connect virtually with people who share similar interests. They are platforms that people use to make friends, build social communities, and connect virtually with people who share similar interests. The paper addresses the issue of fake user profiles in Online Social Networks (OSNs) by proposing a novel classification method based on Random forest. The method aims to detect genuine user profiles by comparing them with fake profiles generated by bots. The study evaluates three datasets: Facebook, Instagram, and Twitter. It finds that Random forest performs better in detecting genuine user profiles compared to other algorithms. The paper concludes that Random forest is the most The paper addresses the challenge of identifying fake accounts in social networks by proposing a novel approach using Linear Logistic Regression (LNR) and DeepProfile to identify the fake users. The dataset consists of 1500 users being fake accounts and remaining 1500 normal accounts. The method was evaluated using Twitter dataset and found that Medium had the highest accuracy of 97.6%. The study highlights the effectiveness of LNR and DeepProfile in identifying the fake users when a This paper presents a novel method for detecting fake accounts on Facebook using the Deep-Profile network. By using the WalkPool function with the Deep-Profile network, the method was tested on a dataset of 549 accounts and 2672 assumed real users. The results showed that the Deep-Profile network achieved the highest accuracy of 79%, outperforming other methods like Random Tree, Naive Bayes, k-Nearest Neighb This paper presents a novel method for detecting fake or cloned accounts in Twitter. The dataset was collected from projects, this was a mixture of social experts, researchers and journalists. An accuracy of ninety percent was achieved by the proposed model. The work can be extended by considering the tweets using techniques. The paper highlights the effectiveness of this method and suggests future work on recurrent neural networks for better results. This paper presents a novel method for manually selecting the best attribute from a dataset. The method involves selecting the attributes from the original dataset and comparing them to the best attribute selected by the model. The results show that manually selecting the best attribute achieves the highest accuracy. The paper also discusses the challenges in manually selecting the best attribute from a dataset. The paper highlights the need for further research in this area to enhance the accuracy of feature selection algorithms and improve the performance of machine This paper presents a novel method for classifying social media users based on their age, gender, and social network status. The method involves three stages: training, testing, and automated feature selection. The training stage involves selecting attributes from the Facebook dataset, while the testing stage involves selecting attributes from the Instagram dataset and the automated feature selection stage involves manually selecting features from the Twitter dataset. The results show that Random Forest achieves the highest accuracy of 94% for the Facebook dataset, while the This paper proposes a classification model to identify genuine and not genuine users on social media platforms like Facebook, Instagram, and Twitter. It uses a combination of Neural Network and Random Forest algorithms to detect genuine users and not fake users. The model is tested on three datasets: Recall.91 0.90 90% 0.90 Neural Network 0.94 0.93 93% 0.93 Random Forest 0.95 0.94 94%. The paper concludes that Random Forest performs best with an The paper explores the role of social networks in enhancing the quality of life for vulnerable populations, particularly those living in rural areas. It highlights the importance of social networks in improving access to healthcare, education, and employment, and suggests that further research is needed to better understand their impact on quality of life.\n",
            "\n",
            "ROUGE Scores:\n",
            " {'rouge1': Score(precision=0.09752438109527382, recall=0.7878787878787878, fmeasure=0.17356475300400534), 'rouge2': Score(precision=0.05105105105105105, recall=0.4146341463414634, fmeasure=0.0909090909090909), 'rougeL': Score(precision=0.06301575393848462, recall=0.509090909090909, fmeasure=0.11214953271028036)}\n"
          ]
        }
      ],
      "source": [
        "# Install necessary libraries\n",
        "!pip install transformers[torch] accelerate -U pdfplumber rouge_score\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Import libraries\n",
        "import pdfplumber\n",
        "from transformers import PegasusTokenizer, PegasusForConditionalGeneration, T5Tokenizer, T5ForConditionalGeneration\n",
        "import torch\n",
        "import re\n",
        "from google.colab import files  # For uploading files in Google Colab\n",
        "from rouge_score import rouge_scorer\n",
        "\n",
        "# Load Pegasus model and tokenizer\n",
        "pegasus_model_path = \"/content/drive/MyDrive/new dataset/model_v2\"\n",
        "pegasus_tokenizer = PegasusTokenizer.from_pretrained(pegasus_model_path)\n",
        "pegasus_model = PegasusForConditionalGeneration.from_pretrained(pegasus_model_path)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "pegasus_model.to(device)\n",
        "\n",
        "# Load T5 model and tokenizer\n",
        "t5_model_path = \"/content/drive/MyDrive/new dataset/model_t5_v2\"\n",
        "t5_tokenizer = T5Tokenizer.from_pretrained(t5_model_path)\n",
        "t5_model = T5ForConditionalGeneration.from_pretrained(t5_model_path)\n",
        "t5_model.to(device)\n",
        "\n",
        "# Define the reference summary directly in the code\n",
        "reference_summary = \"\"\"\n",
        "This paper focuses on the growing importance of identifying genuine user profiles on online social networks (OSNs) like Facebook, Instagram, and Twitter due to the rise in cyber frauds through fake accounts. OSNs, while enhancing virtual communication, expose users to threats like fake profiles, phishing, and trolls, which may lead to misuse of personal information or damage reputations.\n",
        "\n",
        "To address this issue, the paper proposes a machine learning-based classification model that distinguishes between genuine and non-genuine profiles. The model uses datasets from Facebook, Instagram, and Twitter, which undergo preprocessing and feature extraction before being classified using algorithms such as Support Vector Machine (SVM), Neural Network, and Random Forest.\n",
        "\n",
        "The results show that the Random Forest algorithm outperformed others, achieving the highest accuracy of 95% across all datasets. The model's performance is evaluated using metrics like recall, precision, and accuracy. The study concludes that an effective classification model can help safeguard users from cyber frauds, with Random Forest being the most reliable classifie.\n",
        "\"\"\"\n",
        "\n",
        "# Function to extract text from multi-column PDF\n",
        "def extract_text_from_columns(pdf_path):\n",
        "    text = \"\"\n",
        "    with pdfplumber.open(pdf_path) as pdf:\n",
        "        for page in pdf.pages:\n",
        "            left_bbox = (0, 0, page.width / 2, page.height)\n",
        "            right_bbox = (page.width / 2, 0, page.width, page.height)\n",
        "\n",
        "            left_text = page.within_bbox(left_bbox).extract_text()\n",
        "            right_text = page.within_bbox(right_bbox).extract_text()\n",
        "\n",
        "            combined_text = (left_text or '') + ' ' + (right_text or '')\n",
        "            text += combined_text + '\\n'\n",
        "    return text\n",
        "\n",
        "# Function to clean text by removing headers, footers, and references\n",
        "def clean_text(text):\n",
        "    cleaned_text = re.sub(r'\\b(?:[A-Z][A-Z0-9 ]+|Page \\d+|Header|Footer)\\b', '', text, flags=re.MULTILINE)\n",
        "    cleaned_text = re.sub(r'\\bReferences\\b.*$', '', cleaned_text, flags=re.S)\n",
        "    return cleaned_text\n",
        "\n",
        "# Function to extract sections from the cleaned text\n",
        "def extract_section(text, section_title):\n",
        "    pattern = rf'{section_title}[\\s\\S]*?(?=\\n[A-Z][A-Z\\s]+:|$)'\n",
        "    match = re.search(pattern, text, re.IGNORECASE)\n",
        "    if match:\n",
        "        return match.group().strip()\n",
        "    return ''\n",
        "\n",
        "# Function to chunk text\n",
        "def chunk_text(text, max_length=512):\n",
        "    tokens = pegasus_tokenizer.encode(text, truncation=False)\n",
        "    chunks = [tokens[i:i + max_length] for i in range(0, len(tokens), max_length)]\n",
        "    return chunks\n",
        "\n",
        "# Function to generate summaries using Pegasus\n",
        "def generate_summary_pegasus(text_chunk, max_length=100):\n",
        "    inputs = pegasus_tokenizer(text_chunk, return_tensors=\"pt\", max_length=512, truncation=True, padding=\"longest\")\n",
        "    inputs = {key: value.to(device) for key, value in inputs.items()}\n",
        "    summary_ids = pegasus_model.generate(\n",
        "        inputs[\"input_ids\"],\n",
        "        max_length=max_length,\n",
        "        min_length=30,\n",
        "        length_penalty=2.0,\n",
        "        num_beams=4,\n",
        "        early_stopping=True\n",
        "    )\n",
        "    summary = pegasus_tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "    return summary\n",
        "\n",
        "# Function to refine summaries using T5\n",
        "def refine_summary_t5(pegasus_summary, max_length=100):\n",
        "    inputs = t5_tokenizer(pegasus_summary, return_tensors=\"pt\", max_length=512, truncation=True, padding=\"longest\")\n",
        "    inputs = {key: value.to(device) for key, value in inputs.items()}\n",
        "    summary_ids = t5_model.generate(\n",
        "        inputs[\"input_ids\"],\n",
        "        max_length=max_length,\n",
        "        min_length=30,\n",
        "        length_penalty=2.0,\n",
        "        num_beams=4,\n",
        "        early_stopping=True\n",
        "    )\n",
        "    refined_summary = t5_tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "    return refined_summary\n",
        "\n",
        "# Function to clean and ensure proper sentence boundaries\n",
        "def clean_summary(summary):\n",
        "    sentences = sent_tokenize(summary)\n",
        "\n",
        "    unique_sentences = []\n",
        "    sentence_count = {}\n",
        "\n",
        "    for sentence in sentences:\n",
        "        sentence = sentence.strip()\n",
        "        if len(sentence) > 0 and (sentence[-1] not in '.!?'):\n",
        "            sentence += '.'\n",
        "\n",
        "        if sentence in sentence_count:\n",
        "            sentence_count[sentence] += 1\n",
        "        else:\n",
        "            sentence_count[sentence] = 1\n",
        "\n",
        "        if sentence_count[sentence] <= 2:\n",
        "            unique_sentences.append(sentence)\n",
        "\n",
        "    cleaned_summary = ' '.join(unique_sentences)\n",
        "    return cleaned_summary\n",
        "\n",
        "# Function to summarize large text\n",
        "def summarize_large_text(text):\n",
        "    chunks = chunk_text(text)\n",
        "    pegasus_summaries = [generate_summary_pegasus(pegasus_tokenizer.decode(chunk)) for chunk in chunks]\n",
        "    refined_summaries = [refine_summary_t5(summary) for summary in pegasus_summaries]\n",
        "\n",
        "    combined_summary = ' '.join(refined_summaries)\n",
        "    final_summary = clean_summary(combined_summary)\n",
        "\n",
        "    return final_summary\n",
        "\n",
        "# Function to calculate ROUGE score\n",
        "def calculate_rouge(reference, generated):\n",
        "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
        "    scores = scorer.score(reference, generated)\n",
        "    return scores\n",
        "\n",
        "# Main function to process PDF and summarize\n",
        "def main():\n",
        "    print(\"Welcome to the PDF Summarizer!\")\n",
        "    uploaded = files.upload()  # User uploads the PDF file\n",
        "\n",
        "    for pdf_name in uploaded.keys():\n",
        "        print(f\"Processing file: {pdf_name}\")\n",
        "        document_text = extract_text_from_columns(pdf_name)\n",
        "        cleaned_text = clean_text(document_text)\n",
        "\n",
        "        print(\"Extracting relevant sections...\")\n",
        "        abstract_text = extract_section(cleaned_text, 'Abstract')\n",
        "        results_text = extract_section(cleaned_text, 'Results')\n",
        "        methodology_text = extract_section(cleaned_text, 'Methodology')\n",
        "        conclusion_text = extract_section(cleaned_text, 'Conclusion')\n",
        "        introduction_text = extract_section(cleaned_text, 'Introduction')\n",
        "\n",
        "        combined_text = f\"{introduction_text}\\n{abstract_text}\\n{results_text}\\n{methodology_text}\\n{conclusion_text}\"\n",
        "\n",
        "        print(\"Generating summary...\")\n",
        "        combined_summary = summarize_large_text(combined_text)\n",
        "\n",
        "        print(\"\\nFinal Summary:\\n\", combined_summary)\n",
        "\n",
        "        # Calculate ROUGE scores\n",
        "        rouge_scores = calculate_rouge(reference_summary, combined_summary)\n",
        "\n",
        "        print(\"\\nROUGE Scores:\\n\", rouge_scores)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VRAJKNJFJOx8"
      },
      "source": [
        "final code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 852
        },
        "id": "nKyGqZQ6JQox",
        "outputId": "dc7c149e-a2c9-42a4-d054-be80030f4d45"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (0.34.2)\n",
            "Requirement already satisfied: pdfplumber in /usr/local/lib/python3.10/dist-packages (0.11.4)\n",
            "Requirement already satisfied: transformers[torch] in /usr/local/lib/python3.10/dist-packages (4.44.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (3.16.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.24.7)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (4.66.5)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.4.0+cu121)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pdfminer.six==20231228 in /usr/local/lib/python3.10/dist-packages (from pdfplumber) (20231228)\n",
            "Requirement already satisfied: Pillow>=9.1 in /usr/local/lib/python3.10/dist-packages (from pdfplumber) (9.4.0)\n",
            "Requirement already satisfied: pypdfium2>=4.18.0 in /usr/local/lib/python3.10/dist-packages (from pdfplumber) (4.30.0)\n",
            "Requirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six==20231228->pdfplumber) (3.3.2)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six==20231228->pdfplumber) (43.0.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers[torch]) (2024.6.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers[torch]) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (1.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (3.1.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2024.8.30)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber) (1.17.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->transformers[torch]) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->transformers[torch]) (1.3.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber) (2.22)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Welcome to the PDF Summarizer!\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-1545ea5a-9f79-427d-b4e5-61dfc5d2dea1\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-1545ea5a-9f79-427d-b4e5-61dfc5d2dea1\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving sodapdf-converted (1).pdf to sodapdf-converted (1).pdf\n",
            "Processing file: sodapdf-converted (1).pdf\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (1940 > 512). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting relevant sections...\n",
            "Generating summary...\n",
            "\n",
            "Final Summary:\n",
            " The paper addresses the challenge of detecting fake profiles on online social networks (OSNs) by developing a machine learning model that focuses on identifying genuine users rather than fake categories. The study highlights the importance of online social networks in changing people's views on social life during times of pandemics and global lockdowns. It proposes a machine learning-based approach to identify fake profiles, which are often used by hackers to gain access to sensitive information. The The paper addresses the challenge of detecting genuine user profiles on social networking sites, which have become increasingly popular due to their ease of use. It proposes a model based on Bayesian inference, which aims to identify and classify user profiles as genuine or ws. The proposed model achieved an average of all three datasets. The study highlights the importance of detecting genuine user profiles on social networking sites, which have become increasingly popular due to their ease of use This paper presents a novel approach to the classification of user profiles on social media networks using Random Fores (RfF). RfF is a method that uses random fores to estimate the likelihood of a user being genuine or ws that the proposed model achieved an average all three datasets. The paper emphasizes the importance of Random Fores in online social networks, where users can create personal profiles and share their interests with others. The paper highlights the Social networking sites like Facebook, LinkedIn, Orkut, MySpace, and Twitter are used by millions of people to connect and share personal information. They allow users to share information about themselves with others who are similar to them, such as friends, family, and coworkers. They also allow users to share information about themselves with others who are similar to them, such as friends, family, and coworkers. Social networking sites like Facebook, LinkedIn, Orkut, MySpace.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "!pip install transformers[torch] accelerate -U pdfplumber\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "nltk.download('punkt')\n",
        "\n",
        "\n",
        "import pdfplumber\n",
        "from transformers import PegasusTokenizer, PegasusForConditionalGeneration, T5Tokenizer, T5ForConditionalGeneration\n",
        "import torch\n",
        "import re\n",
        "\n",
        "pegasus_model_path = \"/content/drive/MyDrive/new dataset/model_v2\"\n",
        "pegasus_tokenizer = PegasusTokenizer.from_pretrained(pegasus_model_path)\n",
        "pegasus_model = PegasusForConditionalGeneration.from_pretrained(pegasus_model_path)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "pegasus_model.to(device)\n",
        "\n",
        "\n",
        "t5_model_path = \"/content/drive/MyDrive/new dataset/model_t5_v2\"\n",
        "t5_tokenizer = T5Tokenizer.from_pretrained(t5_model_path)\n",
        "t5_model = T5ForConditionalGeneration.from_pretrained(t5_model_path)\n",
        "t5_model.to(device)\n",
        "\n",
        "\n",
        "def extract_text_from_columns(pdf_path):\n",
        "    text = \"\"\n",
        "    with pdfplumber.open(pdf_path) as pdf:\n",
        "        for page in pdf.pages:\n",
        "            left_bbox = (0, 0, page.width / 2, page.height)\n",
        "            right_bbox = (page.width / 2, 0, page.width, page.height)\n",
        "\n",
        "            left_text = page.within_bbox(left_bbox).extract_text()\n",
        "            right_text = page.within_bbox(right_bbox).extract_text()\n",
        "\n",
        "            combined_text = (left_text or '') + ' ' + (right_text or '')\n",
        "            text += combined_text + '\\n'\n",
        "    return text\n",
        "\n",
        "\n",
        "def clean_text(text):\n",
        "    cleaned_text = re.sub(r'\\b(?:[A-Z][A-Z0-9 ]+|Page \\d+|Header|Footer)\\b', '', text, flags=re.MULTILINE)\n",
        "    cleaned_text = re.sub(r'\\bReferences\\b.*$', '', cleaned_text, flags=re.S)\n",
        "    return cleaned_text\n",
        "\n",
        "\n",
        "def extract_section(text, section_title):\n",
        "    pattern = rf'{section_title}[\\s\\S]*?(?=\\n[A-Z][A-Z\\s]+:|$)'\n",
        "    match = re.search(pattern, text, re.IGNORECASE)\n",
        "    if match:\n",
        "        return match.group().strip()\n",
        "    return ''\n",
        "\n",
        "\n",
        "def chunk_text(text, max_length=512):\n",
        "    tokens = pegasus_tokenizer.encode(text, truncation=False)\n",
        "    chunks = [tokens[i:i + max_length] for i in range(0, len(tokens), max_length)]\n",
        "    return chunks\n",
        "\n",
        "\n",
        "def generate_summary_pegasus(text_chunk, max_length=100):\n",
        "    inputs = pegasus_tokenizer(text_chunk, return_tensors=\"pt\", max_length=512, truncation=True, padding=\"longest\")\n",
        "    inputs = {key: value.to(device) for key, value in inputs.items()}\n",
        "    summary_ids = pegasus_model.generate(\n",
        "        inputs[\"input_ids\"],\n",
        "        max_length=max_length,\n",
        "        min_length=30,\n",
        "        length_penalty=2.0,\n",
        "        num_beams=4,\n",
        "        early_stopping=True\n",
        "    )\n",
        "    summary = pegasus_tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "    return summary\n",
        "\n",
        "\n",
        "def refine_summary_t5(pegasus_summary, max_length=100):\n",
        "    inputs = t5_tokenizer(pegasus_summary, return_tensors=\"pt\", max_length=512, truncation=True, padding=\"longest\")\n",
        "    inputs = {key: value.to(device) for key, value in inputs.items()}\n",
        "    summary_ids = t5_model.generate(\n",
        "        inputs[\"input_ids\"],\n",
        "        max_length=max_length,\n",
        "        min_length=30,\n",
        "        length_penalty=2.0,\n",
        "        num_beams=4,\n",
        "        early_stopping=True\n",
        "    )\n",
        "    refined_summary = t5_tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "    return refined_summary\n",
        "\n",
        "\n",
        "def clean_summary(summary):\n",
        "    sentences = sent_tokenize(summary)\n",
        "\n",
        "    unique_sentences = []\n",
        "    sentence_count = {}\n",
        "\n",
        "    for sentence in sentences:\n",
        "        sentence = sentence.strip()\n",
        "        if len(sentence) > 0 and (sentence[-1] not in '.!?'):\n",
        "            sentence += '.'\n",
        "\n",
        "        if sentence in sentence_count:\n",
        "            sentence_count[sentence] += 1\n",
        "        else:\n",
        "            sentence_count[sentence] = 1\n",
        "\n",
        "        if sentence_count[sentence] <= 2:\n",
        "            unique_sentences.append(sentence)\n",
        "\n",
        "    cleaned_summary = ' '.join(unique_sentences)\n",
        "    return cleaned_summary\n",
        "\n",
        "\n",
        "def summarize_large_text(text):\n",
        "    chunks = chunk_text(text)\n",
        "    pegasus_summaries = [generate_summary_pegasus(pegasus_tokenizer.decode(chunk)) for chunk in chunks]\n",
        "    refined_summaries = [refine_summary_t5(summary) for summary in pegasus_summaries]\n",
        "\n",
        "    combined_summary = ' '.join(refined_summaries)\n",
        "    final_summary = clean_summary(combined_summary)\n",
        "\n",
        "    return final_summary\n",
        "\n",
        "\n",
        "def main():\n",
        "    print(\"Welcome to the PDF Summarizer!\")\n",
        "    uploaded = files.upload()\n",
        "\n",
        "    for pdf_name in uploaded.keys():\n",
        "        print(f\"Processing file: {pdf_name}\")\n",
        "        document_text = extract_text_from_columns(pdf_name)\n",
        "        cleaned_text = clean_text(document_text)\n",
        "\n",
        "        print(\"Extracting relevant sections...\")\n",
        "        abstract_text = extract_section(cleaned_text, 'Abstract')\n",
        "        results_text = extract_section(cleaned_text, 'Results')\n",
        "        methodology_text = extract_section(cleaned_text, 'Methodology')\n",
        "        conclusion_text = extract_section(cleaned_text, 'Conclusion')\n",
        "        introduction_text = extract_section(cleaned_text, 'Introduction')\n",
        "\n",
        "        combined_text = f\"{introduction_text}\\n{abstract_text}\\n{results_text}\\n{methodology_text}\\n{conclusion_text}\"\n",
        "\n",
        "        print(\"Generating summary...\")\n",
        "        combined_summary = summarize_large_text(combined_text)\n",
        "\n",
        "        print(\"\\nFinal Summary:\\n\", combined_summary)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QovX39EcWkVS"
      },
      "source": [
        "new"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "duEzc6MUWmhy",
        "outputId": "4fdaa9ff-18ab-43c8-f130-3b114874b94f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.5.2)\n",
            "Requirement already satisfied: pdfplumber in /usr/local/lib/python3.11/dist-packages (0.11.5)\n",
            "Requirement already satisfied: transformers[torch] in /usr/local/lib/python3.11/dist-packages (4.49.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers[torch]) (3.17.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in /usr/local/lib/python3.11/dist-packages (from transformers[torch]) (0.28.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers[torch]) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers[torch]) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers[torch]) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers[torch]) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers[torch]) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers[torch]) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers[torch]) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers[torch]) (4.67.1)\n",
            "Requirement already satisfied: torch>=2.0 in /usr/local/lib/python3.11/dist-packages (from transformers[torch]) (2.6.0+cu124)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pdfminer.six==20231228 in /usr/local/lib/python3.11/dist-packages (from pdfplumber) (20231228)\n",
            "Requirement already satisfied: Pillow>=9.1 in /usr/local/lib/python3.11/dist-packages (from pdfplumber) (11.1.0)\n",
            "Requirement already satisfied: pypdfium2>=4.18.0 in /usr/local/lib/python3.11/dist-packages (from pdfplumber) (4.30.1)\n",
            "Requirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from pdfminer.six==20231228->pdfplumber) (3.4.1)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.11/dist-packages (from pdfminer.six==20231228->pdfplumber) (43.0.3)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.26.0->transformers[torch]) (2024.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.26.0->transformers[torch]) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->transformers[torch]) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->transformers[torch]) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->transformers[torch]) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->transformers[torch]) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->transformers[torch]) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->transformers[torch]) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->transformers[torch]) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->transformers[torch]) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->transformers[torch]) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->transformers[torch]) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->transformers[torch]) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->transformers[torch]) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->transformers[torch]) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->transformers[torch]) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->transformers[torch]) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->transformers[torch]) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->transformers[torch]) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0->transformers[torch]) (1.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers[torch]) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers[torch]) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers[torch]) (2025.1.31)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.11/dist-packages (from cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber) (1.17.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0->transformers[torch]) (3.0.2)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber) (2.22)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "ename": "LookupError",
          "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-5cdb2b08bdd0>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msection_title\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msections\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 155\u001b[0;31m     \u001b[0msection_summary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msummarize_large_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    156\u001b[0m     \u001b[0msection_summaries\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msection_summary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-5cdb2b08bdd0>\u001b[0m in \u001b[0;36msummarize_large_text\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;31m# Function to summarize large text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msummarize_large_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m     \u001b[0mchunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchunk_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m     \u001b[0mpegasus_summaries\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mgenerate_summary_pegasus\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mchunks\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0mrefined_summaries\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mrefine_summary_t5\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msummary\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpegasus_summaries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-5cdb2b08bdd0>\u001b[0m in \u001b[0;36mchunk_text\u001b[0;34m(text, max_length)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;31m# Function to chunk text by sentence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mchunk_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m     \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m     \u001b[0mchunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0mcurrent_chunk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mparam\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mPunkt\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \"\"\"\n\u001b[0;32m--> 119\u001b[0;31m     \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_punkt_tokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36m_get_punkt_tokenizer\u001b[0;34m(language)\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mtype\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \"\"\"\n\u001b[0;32m--> 105\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mPunktTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, lang)\u001b[0m\n\u001b[1;32m   1742\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlang\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"english\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1743\u001b[0m         \u001b[0mPunktSentenceTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1744\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_lang\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1745\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mload_lang\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlang\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"english\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36mload_lang\u001b[0;34m(self, lang)\u001b[0m\n\u001b[1;32m   1747\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfind\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1749\u001b[0;31m         \u001b[0mlang_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"tokenizers/punkt_tab/{lang}/\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1750\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_punkt_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlang_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1751\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lang\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlang\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    577\u001b[0m     \u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"*\"\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m70\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"\\n{sep}\\n{msg}\\n{sep}\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 579\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    580\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n"
          ]
        }
      ],
      "source": [
        "# Install necessary libraries\n",
        "!pip install transformers[torch] accelerate -U pdfplumber\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Import libraries\n",
        "import pdfplumber\n",
        "from transformers import PegasusTokenizer, PegasusForConditionalGeneration, T5Tokenizer, T5ForConditionalGeneration\n",
        "import torch\n",
        "import re\n",
        "\n",
        "# Load Pegasus model and tokenizer\n",
        "pegasus_model_path = \"/content/drive/MyDrive/new dataset/model_v2\"\n",
        "pegasus_tokenizer = PegasusTokenizer.from_pretrained(pegasus_model_path)\n",
        "pegasus_model = PegasusForConditionalGeneration.from_pretrained(pegasus_model_path)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "pegasus_model.to(device)\n",
        "\n",
        "# Load T5 model and tokenizer\n",
        "t5_model_path = \"/content/drive/MyDrive/new dataset/model_t5_v2\"\n",
        "t5_tokenizer = T5Tokenizer.from_pretrained(t5_model_path)\n",
        "t5_model = T5ForConditionalGeneration.from_pretrained(t5_model_path)\n",
        "t5_model.to(device)\n",
        "\n",
        "# Function to extract text from multi-column PDF\n",
        "def extract_text_from_columns(pdf_path):\n",
        "    text = \"\"\n",
        "    with pdfplumber.open(pdf_path) as pdf:\n",
        "        for page in pdf.pages:\n",
        "            left_bbox = (0, 0, page.width / 2, page.height)\n",
        "            right_bbox = (page.width / 2, 0, page.width, page.height)\n",
        "\n",
        "            left_text = page.within_bbox(left_bbox).extract_text()\n",
        "            right_text = page.within_bbox(right_bbox).extract_text()\n",
        "\n",
        "            combined_text = (left_text or '') + ' ' + (right_text or '')\n",
        "            text += combined_text + '\\n'\n",
        "    return text\n",
        "\n",
        "# Function to clean text by removing headers, footers, and references\n",
        "def clean_text(text):\n",
        "    cleaned_text = re.sub(r'\\b(?:[A-Z][A-Z0-9 ]+|Page \\d+|Header|Footer)\\b', '', text, flags=re.MULTILINE)\n",
        "    cleaned_text = re.sub(r'\\bReferences\\b.*$', '', cleaned_text, flags=re.S)\n",
        "    return cleaned_text\n",
        "\n",
        "# Function to extract sections from the cleaned text\n",
        "def extract_section(text, section_title):\n",
        "    pattern = rf'{section_title}[\\s\\S]*?(?=\\n[A-Z][A-Z\\s]+:|$)'\n",
        "    match = re.search(pattern, text, re.IGNORECASE)\n",
        "    if match:\n",
        "        return match.group().strip()\n",
        "    return ''\n",
        "\n",
        "# Function to chunk text by sentence\n",
        "def chunk_text(text, max_length=512):\n",
        "    sentences = sent_tokenize(text)\n",
        "    chunks = []\n",
        "    current_chunk = \"\"\n",
        "\n",
        "    for sentence in sentences:\n",
        "        if len(pegasus_tokenizer.encode(current_chunk + sentence)) < max_length:\n",
        "            current_chunk += sentence + \" \"\n",
        "        else:\n",
        "            chunks.append(current_chunk.strip())\n",
        "            current_chunk = sentence + \" \"\n",
        "\n",
        "    if current_chunk:\n",
        "        chunks.append(current_chunk.strip())\n",
        "\n",
        "    return chunks\n",
        "\n",
        "# Function to generate summaries using Pegasus\n",
        "def generate_summary_pegasus(text_chunk, max_length=100):\n",
        "    inputs = pegasus_tokenizer(text_chunk, return_tensors=\"pt\", max_length=512, truncation=True, padding=\"longest\")\n",
        "    inputs = {key: value.to(device) for key, value in inputs.items()}\n",
        "    summary_ids = pegasus_model.generate(\n",
        "        inputs[\"input_ids\"],\n",
        "        max_length=max_length,\n",
        "        min_length=30,\n",
        "        length_penalty=2.0,\n",
        "        num_beams=4,\n",
        "        early_stopping=True\n",
        "    )\n",
        "    summary = pegasus_tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "    return summary\n",
        "\n",
        "# Function to refine summaries using T5\n",
        "def refine_summary_t5(pegasus_summary, max_length=100):\n",
        "    inputs = t5_tokenizer(pegasus_summary, return_tensors=\"pt\", max_length=512, truncation=True, padding=\"longest\")\n",
        "    inputs = {key: value.to(device) for key, value in inputs.items()}\n",
        "    summary_ids = t5_model.generate(\n",
        "        inputs[\"input_ids\"],\n",
        "        max_length=max_length,\n",
        "        min_length=30,\n",
        "        length_penalty=2.0,\n",
        "        num_beams=4,\n",
        "        early_stopping=True\n",
        "    )\n",
        "    refined_summary = t5_tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "    return refined_summary\n",
        "\n",
        "# Function to clean and ensure proper sentence boundaries\n",
        "def clean_summary(summary):\n",
        "    sentences = sent_tokenize(summary)\n",
        "    unique_sentences = []\n",
        "    sentence_count = {}\n",
        "\n",
        "    for sentence in sentences:\n",
        "        sentence = sentence.strip()\n",
        "        if len(sentence) > 0 and (sentence[-1] not in '.!?'):\n",
        "            sentence += '.'\n",
        "        if sentence in sentence_count:\n",
        "            sentence_count[sentence] += 1\n",
        "        else:\n",
        "            sentence_count[sentence] = 1\n",
        "        if sentence_count[sentence] <= 1:\n",
        "            unique_sentences.append(sentence)\n",
        "\n",
        "    cleaned_summary = ' '.join(unique_sentences)\n",
        "    return cleaned_summary\n",
        "\n",
        "# Function to summarize large text\n",
        "def summarize_large_text(text):\n",
        "    chunks = chunk_text(text)\n",
        "    pegasus_summaries = [generate_summary_pegasus(chunk) for chunk in chunks]\n",
        "    refined_summaries = [refine_summary_t5(summary) for summary in pegasus_summaries]\n",
        "    combined_summary = ' '.join(refined_summaries)\n",
        "    final_summary = clean_summary(combined_summary)\n",
        "    return final_summary\n",
        "\n",
        "# Process PDF and generate summaries\n",
        "pdf_path = \"/content/drive/MyDrive/new dataset/mll.pdf\"\n",
        "document_text = extract_text_from_columns(pdf_path)\n",
        "cleaned_text = clean_text(document_text)\n",
        "\n",
        "# Extract sections\n",
        "abstract_text = extract_section(cleaned_text, 'Abstract')\n",
        "results_text = extract_section(cleaned_text, 'Results')\n",
        "methodology_text = extract_section(cleaned_text, 'Methodology')\n",
        "conclusion_text = extract_section(cleaned_text, 'Conclusion')\n",
        "introduction_text = extract_section(cleaned_text, 'Introduction')\n",
        "\n",
        "# Summarize each section individually\n",
        "section_summaries = []\n",
        "sections = [\n",
        "    ('Abstract', abstract_text),\n",
        "    ('Introduction', introduction_text),\n",
        "    ('Methodology', methodology_text),\n",
        "    ('Results', results_text),\n",
        "    ('Conclusion', conclusion_text)\n",
        "]\n",
        "\n",
        "for section_title, text in sections:\n",
        "    section_summary = summarize_large_text(text)\n",
        "    section_summaries.append(section_summary)\n",
        "\n",
        "# Combine the section-wise summaries into a single comprehensive summary\n",
        "combined_summary = \" \".join(section_summaries)\n",
        "\n",
        "# Clean the combined summary to remove duplicate sentences\n",
        "final_combined_summary = clean_summary(combined_summary)\n",
        "\n",
        "# Print the final combined summary\n",
        "print(\"\\nCombined Comprehensive Summary:\\n\")\n",
        "print(final_combined_summary)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LJ81e4lC9c39"
      },
      "source": [
        "FINAL CODE\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "izmdknff9cNl"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u5Rvzp40XYEz",
        "outputId": "49d8b258-a1a6-4092-e1e7-fc13035f4b51"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.9.0)\n",
            "Requirement already satisfied: pdfplumber in /usr/local/lib/python3.11/dist-packages (0.11.7)\n",
            "Requirement already satisfied: rouge-score in /usr/local/lib/python3.11/dist-packages (0.1.2)\n",
            "Requirement already satisfied: transformers[torch] in /usr/local/lib/python3.11/dist-packages (4.53.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers[torch]) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers[torch]) (0.33.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers[torch]) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers[torch]) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers[torch]) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers[torch]) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers[torch]) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers[torch]) (0.21.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers[torch]) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers[torch]) (4.67.1)\n",
            "Requirement already satisfied: torch>=2.1 in /usr/local/lib/python3.11/dist-packages (from transformers[torch]) (2.6.0+cu124)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pdfminer.six==20250506 in /usr/local/lib/python3.11/dist-packages (from pdfplumber) (20250506)\n",
            "Requirement already satisfied: Pillow>=9.1 in /usr/local/lib/python3.11/dist-packages (from pdfplumber) (11.2.1)\n",
            "Requirement already satisfied: pypdfium2>=4.18.0 in /usr/local/lib/python3.11/dist-packages (from pdfplumber) (4.30.1)\n",
            "Requirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from pdfminer.six==20250506->pdfplumber) (3.4.2)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.11/dist-packages (from pdfminer.six==20250506->pdfplumber) (43.0.3)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from rouge-score) (1.4.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (from rouge-score) (3.9.1)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from rouge-score) (1.17.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers[torch]) (2025.3.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers[torch]) (4.14.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers[torch]) (1.1.5)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.1->transformers[torch]) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1->transformers[torch]) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1->transformers[torch]) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1->transformers[torch]) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1->transformers[torch]) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1->transformers[torch]) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1->transformers[torch]) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1->transformers[torch]) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1->transformers[torch]) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1->transformers[torch]) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1->transformers[torch]) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1->transformers[torch]) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1->transformers[torch]) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1->transformers[torch]) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1->transformers[torch]) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1->transformers[torch]) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1->transformers[torch]) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.1->transformers[torch]) (1.3.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk->rouge-score) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk->rouge-score) (1.5.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers[torch]) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers[torch]) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers[torch]) (2025.7.14)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.11/dist-packages (from cryptography>=36.0.0->pdfminer.six==20250506->pdfplumber) (1.17.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.1->transformers[torch]) (3.0.2)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20250506->pdfplumber) (2.22)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (535 > 512). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Combined Comprehensive Summary:\n",
            "\n",
            "This research explores the use of Bayesian Network Model () for the estimation of the presence of estrous cycle in Japanese dairy cows. Through an experiment with 280 Japanese anestrus Holstein dairy cows, it was found that the model achieved high accuracy in finding out the presence of estrous cycle while using suspended likelihood in sample datasets. The study highlights the advantages of Bayesian Network Model () over subjective methods for finding out the presence of estrous cycle in dairy cows. The research also reveals the optimum factors to find out the presence of estrous cycle among the 270 individual dairy cows. The findings suggest that the Bayesian Network Model () with the inclusion of Body Condition Para. This paper presents a novel method for identifying the presence or absence of estrous cycle in cattle using k Model. The method is based on the assumption that the cow has a normal estrous cycle, which is the period from one estrus (heat, phase of sexual receptivity) to the next estrus. The study evaluates the performance of the k Model with respect to the following parameters: Body Condition Score (BCS), Days after childbirth and/or Postpartum number parity, ovarian characteristics, uterine blood flow, progesterone level, climate, and various species of heifers. The results show that the k Model outperforms This study explores the use of Bayesian network model analysis to detect the presence or absence of an estrous cycle in Holstein cows in Japanese dairy industries. The study involved 280 Holstein cows observing with their (2.0 to 3.25), postpartum interval, and parity numbers. It found that, all these factors influence the timing of artificial insemination to make them pregnant. The findings suggest that, using a Bayesian network model, it is possible to identify the optimal time for artificial insemination based on these factors. The study highlights the importance of understanding the role of these factors in determining the optimal time for artificial insemination and suggests future research directions to refine The paper presents a Bayesian network model designed to identify the estrous cycle in cattle using data from the National Health and Nutrition Survey (NHNS). The proposed method uses three types of information: ()/Days after childbirth, Parity number, and Estrous cycle. The results show that the Bayesian network model is more accurate in finding the estrous cycle in relationships with, Postpartum interval, and number of parity. The study highlights the importance of body condition scoring in identifying the estrous cycle, which is the most influential factor in making farm management decisions and most influential for the farm nutritionist. This research focuses on the estimation of estrous cycle in 280 individual 4.0 cattle of Morinaga Dairy Service Co., Ltd., Japan under cooperation with Iwate Prefecture, Japan. The method (0.25 increase) has good repeatability across 4.0 and within observers, including simplified body scoring and have higher value as diagnostic tests. The process represents the observerâ€™s view into the certain anatomical sites for each cowâ€™s pelvic, loin areas, pin and hook bones, and etc. The study highlights the importance of estimating estrous cycle during postpartum anestrus, which is the duration from calving to the subsequent conception interval. The study highlights the importance of This research proposes a Bayesian Network Model to evaluate the finding of estrous cycle while considering, and Parity number. To evaluate the finding of estrous cycle while considering, and Parity number, this research proposed a unique Bayesian Network Model (Fig.3). Each individual cattle is identified with a unique number in the farm and then observed by an experienced inspector. The required, Parity number, and estrous cycle related data were collected and processed according to the proposed model. The interval was 4.0 categorized into 9 groups (Table and ). The model was trained using the Bayonet 6 software, which was developed by Institute Inc. 33 | a ge.thesai. The This paper presents a Bayesian network-based approach for finding the presence of estrous cycle in cattle. The proposed model is based on the estimation of, and Parity number of each cow. The optimal factors for finding the presence of estrous cycle according to, 1 to 4 groups, and Parity number of each cow are, 31-60 days, 91-120 days, and Parity 1 to 4. The results show that the proposed model has the highest estimation accuracy of 93%, while the average accuracy for all data set is almost 55% and the log-likelihood is more than 0.7. The study highlights the benefits of using Bayesian network for finding the presence of estrous cycle in cattle This study investigates the estimation accuracy of the presence of estrous cycle in dairy cattle. It finds that, when the likelihood is 0.5, the highest average estimation accuracy for the presence of estrous cycle is almost 73% (total 40 cattle) and lowest is 50% when the likelihood is considering 0.7. However, when the likelihood is 0.6, most of the dairy cows lies within the likelihood of 0.6 (130 + 40 = 170 cattle) and the average estimation accuracy for considering all cattle is 55%. The research concludes that, when the likelihood is 0.6, most of the dairy cows lies within the likelihood of 0.6 (130 + 40 =170 cattle) and the average estimation accuracy for considering This paper presents a novel method for estimating the presence of estrous cycle in cattle using a Bayesian network model. The study involved 35 cattle from a farm in Missouri, USA, and used a high confidence model to estimate the presence of estrous cycle with respect to body condition parameters, postpartum intervals, and parity. The results showed that the Bayesian network model was more accurate and reliable than traditional methods in finding the most optimal conditions for each individual cattle. The study highlights the effectiveness of this method in identifying the most optimal conditions for each individual cattle to have estrous cycle on due course. The study highlights the effectiveness of this method in identifying the most optimal conditions for each individual cattle This study investigates the impact of ovulatory follicle diameter and progesterone on the productivity of livestock industry in Japan and other countries. The authors propose a method to estimate the presence or absence of estrous cycle concentration in cattle by using ovulatory follicle diameter and progesterone parameters. The method aims to improve livestock productivity by identifying the presence or absence of estrous cycle in cattle. The study highlights the need for further research to validate this method at industry level. The authors suggest that future work should focus on using other parameters of cattle for finding out the presence or absence of estrous cycle. This paper presents a novel method for identifying the presence or absence of an estrous cycle in cattle using k Model. The study evaluates the performance of the k Model with respect to the following parameters: Body Condition Score (BCS), Days after childbirth and or Postpartum, Ovarian Blood Flow (OBF), and Progesterone Level (PL). The study highlights the effectiveness of the k Model in identifying the presence or absence of an estrous cycle in cattle This study explores the use of Bayesian network model analysis to detect the presence or absence of an estrous cycle in Holstein cows in Japanese dairy industries.\n",
            "\n",
            "ROUGE Scores:\n",
            "\n",
            "{'rouge1': Score(precision=0.0, recall=0.0, fmeasure=0.0), 'rouge2': Score(precision=0.0, recall=0.0, fmeasure=0.0), 'rougeL': Score(precision=0, recall=0, fmeasure=0)}\n"
          ]
        }
      ],
      "source": [
        "# Install necessary libraries\n",
        "!pip install transformers[torch] accelerate -U pdfplumber rouge-score\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "# Download necessary NLTK data\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')  # Explicitly downloading punkt_tab to resolve the error\n",
        "\n",
        "\n",
        "# Import libraries\n",
        "import pdfplumber\n",
        "from transformers import PegasusTokenizer, PegasusForConditionalGeneration, T5Tokenizer, T5ForConditionalGeneration\n",
        "import torch\n",
        "import re\n",
        "from rouge_score import rouge_scorer  # For ROUGE score\n",
        "\n",
        "# Load Pegasus model and tokenizer\n",
        "pegasus_model_path = \"/content/drive/MyDrive/new dataset/model_v2\"\n",
        "pegasus_tokenizer = PegasusTokenizer.from_pretrained(pegasus_model_path)\n",
        "pegasus_model = PegasusForConditionalGeneration.from_pretrained(pegasus_model_path)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "pegasus_model.to(device)\n",
        "\n",
        "# Load T5 model and tokenizer\n",
        "t5_model_path = \"/content/drive/MyDrive/new dataset/model_t5_v2\"\n",
        "t5_tokenizer = T5Tokenizer.from_pretrained(t5_model_path)\n",
        "t5_model = T5ForConditionalGeneration.from_pretrained(t5_model_path)\n",
        "t5_model.to(device)\n",
        "\n",
        "# Function to extract text from multi-column PDF\n",
        "def extract_text_from_columns(pdf_path):\n",
        "    text = \"\"\n",
        "    with pdfplumber.open(pdf_path) as pdf:\n",
        "        for page in pdf.pages:\n",
        "            left_bbox = (0, 0, page.width / 2, page.height)\n",
        "            right_bbox = (page.width / 2, 0, page.width, page.height)\n",
        "\n",
        "            left_text = page.within_bbox(left_bbox).extract_text()\n",
        "            right_text = page.within_bbox(right_bbox).extract_text()\n",
        "\n",
        "            combined_text = (left_text or '') + ' ' + (right_text or '')\n",
        "            text += combined_text + '\\n'\n",
        "    return text\n",
        "\n",
        "# Function to clean text by removing headers, footers, and references\n",
        "def clean_text(text):\n",
        "    cleaned_text = re.sub(r'\\b(?:[A-Z][A-Z0-9 ]+|Page \\d+|Header|Footer)\\b', '', text, flags=re.MULTILINE)\n",
        "    cleaned_text = re.sub(r'\\bReferences\\b.*$', '', cleaned_text, flags=re.S)\n",
        "    return cleaned_text\n",
        "\n",
        "# Function to extract sections from the cleaned text\n",
        "def extract_section(text, section_title):\n",
        "    pattern = rf'{section_title}[\\s\\S]*?(?=\\n[A-Z][A-Z\\s]+:|$)'\n",
        "    match = re.search(pattern, text, re.IGNORECASE)\n",
        "    if match:\n",
        "        return match.group().strip()\n",
        "    return ''\n",
        "\n",
        "# Function to chunk text by sentence\n",
        "def chunk_text(text, max_length=512):\n",
        "    sentences = sent_tokenize(text)\n",
        "    chunks = []\n",
        "    current_chunk = \"\"\n",
        "\n",
        "    for sentence in sentences:\n",
        "        if len(pegasus_tokenizer.encode(current_chunk + sentence)) < max_length:\n",
        "            current_chunk += sentence + \" \"\n",
        "        else:\n",
        "            chunks.append(current_chunk.strip())\n",
        "            current_chunk = sentence + \" \"\n",
        "\n",
        "    if current_chunk:\n",
        "        chunks.append(current_chunk.strip())\n",
        "\n",
        "    return chunks\n",
        "\n",
        "# Function to generate summaries using Pegasus\n",
        "def generate_summary_pegasus(text_chunk, max_length=150):\n",
        "    inputs = pegasus_tokenizer(text_chunk, return_tensors=\"pt\", max_length=512, truncation=True, padding=\"longest\")\n",
        "    inputs = {key: value.to(device) for key, value in inputs.items()}\n",
        "    summary_ids = pegasus_model.generate(\n",
        "        inputs[\"input_ids\"],\n",
        "        max_length=max_length,\n",
        "        min_length=80,  # Ensuring enough length\n",
        "        length_penalty=2.0,\n",
        "        num_beams=5,  # Increase beams for more precision\n",
        "        early_stopping=True\n",
        "    )\n",
        "    summary = pegasus_tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "    return summary\n",
        "\n",
        "# Function to refine summaries using T5\n",
        "def refine_summary_t5(pegasus_summary, max_length=150):\n",
        "    inputs = t5_tokenizer(pegasus_summary, return_tensors=\"pt\", max_length=512, truncation=True, padding=\"longest\")\n",
        "    inputs = {key: value.to(device) for key, value in inputs.items()}\n",
        "    summary_ids = t5_model.generate(\n",
        "        inputs[\"input_ids\"],\n",
        "        max_length=max_length,\n",
        "        min_length=80,  # Ensuring refined summary keeps enough details\n",
        "        length_penalty=2.0,\n",
        "        num_beams=5,\n",
        "        early_stopping=True\n",
        "    )\n",
        "    refined_summary = t5_tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "    return refined_summary\n",
        "\n",
        "# Function to clean and ensure proper sentence boundaries\n",
        "def clean_summary(summary):\n",
        "    sentences = sent_tokenize(summary)\n",
        "    unique_sentences = []\n",
        "    sentence_count = {}\n",
        "\n",
        "    for sentence in sentences:\n",
        "        sentence = sentence.strip()\n",
        "        if len(sentence) > 0 and (sentence[-1] not in '.!?'):\n",
        "            sentence += '.'\n",
        "        if sentence in sentence_count:\n",
        "            sentence_count[sentence] += 1\n",
        "        else:\n",
        "            sentence_count[sentence] = 1\n",
        "        if sentence_count[sentence] <= 1:\n",
        "            unique_sentences.append(sentence)\n",
        "\n",
        "    cleaned_summary = ' '.join(unique_sentences)\n",
        "    return cleaned_summary\n",
        "\n",
        "# Function to summarize large text\n",
        "def summarize_large_text(text):\n",
        "    chunks = chunk_text(text)\n",
        "    pegasus_summaries = [generate_summary_pegasus(chunk) for chunk in chunks]\n",
        "    refined_summaries = [refine_summary_t5(summary) for summary in pegasus_summaries]\n",
        "    combined_summary = ' '.join(refined_summaries)\n",
        "    final_summary = clean_summary(combined_summary)\n",
        "    return final_summary\n",
        "\n",
        "# Function to calculate ROUGE score\n",
        "def calculate_rouge(reference_summary, generated_summary):\n",
        "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
        "    scores = scorer.score(reference_summary, generated_summary)\n",
        "    return scores\n",
        "\n",
        "# Process PDF and generate summaries\n",
        "pdf_path = \"/content/drive/MyDrive/new dataset/mll.pdf\"\n",
        "document_text = extract_text_from_columns(pdf_path)\n",
        "cleaned_text = clean_text(document_text)\n",
        "\n",
        "# Extract sections\n",
        "abstract_text = extract_section(cleaned_text, 'Abstract')\n",
        "results_text = extract_section(cleaned_text, 'Results')\n",
        "methodology_text = extract_section(cleaned_text, 'Methodology')\n",
        "conclusion_text = extract_section(cleaned_text, 'Conclusion')\n",
        "introduction_text = extract_section(cleaned_text, 'Introduction')\n",
        "\n",
        "# Summarize each section individually\n",
        "section_summaries = []\n",
        "sections = [\n",
        "    ('Abstract', abstract_text),\n",
        "    ('Introduction', introduction_text),\n",
        "    ('Methodology', methodology_text),\n",
        "    ('Results', results_text),\n",
        "    ('Conclusion', conclusion_text)\n",
        "]\n",
        "\n",
        "for section_title, text in sections:\n",
        "    if text:\n",
        "        section_summary = summarize_large_text(text)\n",
        "        section_summaries.append(section_summary)\n",
        "\n",
        "# Combine the section-wise summaries into a single comprehensive summary\n",
        "combined_summary = \" \".join(section_summaries)\n",
        "\n",
        "# Clean the combined summary to remove duplicate sentences\n",
        "final_combined_summary = clean_summary(combined_summary)\n",
        "\n",
        "# Print the final combined summary\n",
        "print(\"\\nCombined Comprehensive Summary:\\n\")\n",
        "print(final_combined_summary)\n",
        "\n",
        "# Example reference summary (previously generated by ChatGPT for evaluation)\n",
        "reference_summary = '''.'''\n",
        "\n",
        "# Calculate and print ROUGE score\n",
        "rouge_scores = calculate_rouge(reference_summary, final_combined_summary)\n",
        "print(\"\\nROUGE Scores:\\n\")\n",
        "print(rouge_scores)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "eff3d18d41bb4b3e82df8b40be2481c5",
            "0edb57ccadf24698bdfcbce7a2f24e95",
            "4d98ed10a05044718bed04064f84f872",
            "ccdb043398c345799e0ab9d385e6bff7",
            "8058233a09ad447588a3c6526f3514d5",
            "3755aee31ec9464d8c2504c67f41d5f0",
            "19bf33d480874cffbb69e62f7ecb91c1",
            "f28eddaed9fd440a902ac4f52d47e0d7",
            "222ad09a47d44bab936ea5f62d2ba3bf",
            "f7bca2873c774210b306ca3c59b7caa0",
            "3611bbf194e146b48db9d6f7d20d2e9a",
            "3b1ef663e3ba4cec8c2556273f28fa61",
            "67b7991404b14e07840612c17f5ad1dc"
          ]
        },
        "id": "r-m0TXbHA5QF",
        "outputId": "a60b6bca-b5b5-4392-9dee-2857bee9259f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.9.0)\n",
            "Requirement already satisfied: pdfplumber in /usr/local/lib/python3.11/dist-packages (0.11.7)\n",
            "Requirement already satisfied: rouge-score in /usr/local/lib/python3.11/dist-packages (0.1.2)\n",
            "Requirement already satisfied: transformers[torch] in /usr/local/lib/python3.11/dist-packages (4.53.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers[torch]) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers[torch]) (0.33.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers[torch]) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers[torch]) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers[torch]) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers[torch]) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers[torch]) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers[torch]) (0.21.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers[torch]) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers[torch]) (4.67.1)\n",
            "Requirement already satisfied: torch>=2.1 in /usr/local/lib/python3.11/dist-packages (from transformers[torch]) (2.6.0+cu124)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pdfminer.six==20250506 in /usr/local/lib/python3.11/dist-packages (from pdfplumber) (20250506)\n",
            "Requirement already satisfied: Pillow>=9.1 in /usr/local/lib/python3.11/dist-packages (from pdfplumber) (11.2.1)\n",
            "Requirement already satisfied: pypdfium2>=4.18.0 in /usr/local/lib/python3.11/dist-packages (from pdfplumber) (4.30.1)\n",
            "Requirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from pdfminer.six==20250506->pdfplumber) (3.4.2)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.11/dist-packages (from pdfminer.six==20250506->pdfplumber) (43.0.3)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from rouge-score) (1.4.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (from rouge-score) (3.9.1)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from rouge-score) (1.17.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers[torch]) (2025.3.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers[torch]) (4.14.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers[torch]) (1.1.5)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.1->transformers[torch]) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1->transformers[torch]) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1->transformers[torch]) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1->transformers[torch]) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1->transformers[torch]) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1->transformers[torch]) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1->transformers[torch]) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1->transformers[torch]) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1->transformers[torch]) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1->transformers[torch]) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1->transformers[torch]) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1->transformers[torch]) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1->transformers[torch]) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1->transformers[torch]) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1->transformers[torch]) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1->transformers[torch]) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1->transformers[torch]) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.1->transformers[torch]) (1.3.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk->rouge-score) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk->rouge-score) (1.5.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers[torch]) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers[torch]) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers[torch]) (2025.7.14)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.11/dist-packages (from cryptography>=36.0.0->pdfminer.six==20250506->pdfplumber) (1.17.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.1->transformers[torch]) (3.0.2)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20250506->pdfplumber) (2.22)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "eff3d18d41bb4b3e82df8b40be2481c5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(Label(value='Select a research paper from your Google Drive folder (MyDrive/Papers):'), Dropdowâ€¦"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Install necessary libraries\n",
        "!pip install transformers[torch] accelerate -U pdfplumber rouge-score\n",
        "\n",
        "# Import libraries\n",
        "import os\n",
        "import re\n",
        "import nltk\n",
        "import torch\n",
        "import pdfplumber\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from transformers import PegasusTokenizer, PegasusForConditionalGeneration, T5Tokenizer, T5ForConditionalGeneration\n",
        "from rouge_score import rouge_scorer\n",
        "from google.colab import drive\n",
        "\n",
        "# Download necessary NLTK data\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Load Pegasus model and tokenizer\n",
        "pegasus_model_path = \"/content/drive/MyDrive/new dataset/model_v2\"\n",
        "pegasus_tokenizer = PegasusTokenizer.from_pretrained(pegasus_model_path)\n",
        "pegasus_model = PegasusForConditionalGeneration.from_pretrained(pegasus_model_path)\n",
        "\n",
        "# Load T5 model and tokenizer\n",
        "t5_model_path = \"/content/drive/MyDrive/new dataset/model_t5_v2\"\n",
        "t5_tokenizer = T5Tokenizer.from_pretrained(t5_model_path)\n",
        "t5_model = T5ForConditionalGeneration.from_pretrained(t5_model_path)\n",
        "\n",
        "# Setup device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "pegasus_model.to(device)\n",
        "t5_model.to(device)\n",
        "\n",
        "# Function to extract text from multi-column PDF\n",
        "def extract_text_from_columns(pdf_path):\n",
        "    text = \"\"\n",
        "    with pdfplumber.open(pdf_path) as pdf:\n",
        "        for page in pdf.pages:\n",
        "            left_bbox = (0, 0, page.width / 2, page.height)\n",
        "            right_bbox = (page.width / 2, 0, page.width, page.height)\n",
        "            left_text = page.within_bbox(left_bbox).extract_text()\n",
        "            right_text = page.within_bbox(right_bbox).extract_text()\n",
        "            combined_text = (left_text or '') + ' ' + (right_text or '')\n",
        "            text += combined_text + '\\n'\n",
        "    return text\n",
        "\n",
        "# Function to clean text\n",
        "def clean_text(text):\n",
        "    cleaned_text = re.sub(r'\\b(?:[A-Z][A-Z0-9 ]+|Page \\d+|Header|Footer)\\b', '', text, flags=re.MULTILINE)\n",
        "    cleaned_text = re.sub(r'\\bReferences\\b.*$', '', cleaned_text, flags=re.S)\n",
        "    return cleaned_text\n",
        "\n",
        "# Function to extract sections\n",
        "def extract_section(text, section_title):\n",
        "    pattern = rf'{section_title}[\\s\\S]*?(?=\\n[A-Z][A-Z\\s]+:|$)'\n",
        "    match = re.search(pattern, text, re.IGNORECASE)\n",
        "    return match.group().strip() if match else ''\n",
        "\n",
        "# Chunk text into token-length suitable for Pegasus\n",
        "def chunk_text(text, max_length=512):\n",
        "    sentences = sent_tokenize(text)\n",
        "    chunks, current_chunk = [], \"\"\n",
        "    for sentence in sentences:\n",
        "        if len(pegasus_tokenizer.encode(current_chunk + sentence)) < max_length:\n",
        "            current_chunk += sentence + \" \"\n",
        "        else:\n",
        "            chunks.append(current_chunk.strip())\n",
        "            current_chunk = sentence + \" \"\n",
        "    if current_chunk:\n",
        "        chunks.append(current_chunk.strip())\n",
        "    return chunks\n",
        "\n",
        "# Generate summary using Pegasus\n",
        "def generate_summary_pegasus(text_chunk, max_length=150):\n",
        "    inputs = pegasus_tokenizer(text_chunk, return_tensors=\"pt\", max_length=512, truncation=True, padding=\"longest\")\n",
        "    inputs = {key: value.to(device) for key, value in inputs.items()}\n",
        "    summary_ids = pegasus_model.generate(inputs[\"input_ids\"], max_length=max_length, min_length=80, length_penalty=2.0, num_beams=5, early_stopping=True)\n",
        "    return pegasus_tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "\n",
        "# Refine summary using T5\n",
        "def refine_summary_t5(summary, max_length=150):\n",
        "    inputs = t5_tokenizer(summary, return_tensors=\"pt\", max_length=512, truncation=True, padding=\"longest\")\n",
        "    inputs = {key: value.to(device) for key, value in inputs.items()}\n",
        "    summary_ids = t5_model.generate(inputs[\"input_ids\"], max_length=max_length, min_length=80, length_penalty=2.0, num_beams=5, early_stopping=True)\n",
        "    return t5_tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "\n",
        "# Clean final summary\n",
        "def clean_summary(summary):\n",
        "    sentences = sent_tokenize(summary)\n",
        "    seen, cleaned = set(), []\n",
        "    for s in sentences:\n",
        "        s = s.strip()\n",
        "        if s and s not in seen:\n",
        "            seen.add(s)\n",
        "            if s[-1] not in \".!?\":\n",
        "                s += \".\"\n",
        "            cleaned.append(s)\n",
        "    return ' '.join(cleaned)\n",
        "\n",
        "# Summarize large text by chunking\n",
        "def summarize_large_text(text):\n",
        "    chunks = chunk_text(text)\n",
        "    pegasus_summaries = [generate_summary_pegasus(chunk) for chunk in chunks]\n",
        "    refined = [refine_summary_t5(s) for s in pegasus_summaries]\n",
        "    return clean_summary(' '.join(refined))\n",
        "\n",
        "# Calculate ROUGE\n",
        "def calculate_rouge(reference, generated):\n",
        "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
        "    return scorer.score(reference, generated)\n",
        "\n",
        "# Path where papers are stored in Drive\n",
        "papers_folder = \"/content/drive/MyDrive/Papers\"\n",
        "\n",
        "# Ensure folder exists or provide fallback\n",
        "if not os.path.exists(papers_folder):\n",
        "    os.makedirs(papers_folder)\n",
        "    print(f\"Created folder: {papers_folder}. Please upload PDFs to this folder via Google Drive.\")\n",
        "\n",
        "pdf_files = [f for f in os.listdir(papers_folder) if f.endswith(\".pdf\")]\n",
        "\n",
        "# UI Elements\n",
        "file_dropdown = widgets.Dropdown(options=pdf_files, description='Paper:')\n",
        "summarize_button = widgets.Button(description=\"Summarize\", button_style=\"success\")\n",
        "output_area = widgets.Output()\n",
        "\n",
        "# Summarization handler\n",
        "def on_summarize_clicked(b):\n",
        "    output_area.clear_output()\n",
        "    with output_area:\n",
        "        if not file_dropdown.value:\n",
        "            print(\"Please select a PDF file.\")\n",
        "            return\n",
        "        file_path = os.path.join(papers_folder, file_dropdown.value)\n",
        "        print(f\"Processing file: {file_path}\")\n",
        "        raw_text = extract_text_from_columns(file_path)\n",
        "        cleaned = clean_text(raw_text)\n",
        "\n",
        "        sections = [\n",
        "            ('Abstract', extract_section(cleaned, 'Abstract')),\n",
        "            ('Introduction', extract_section(cleaned, 'Introduction')),\n",
        "            ('Methodology', extract_section(cleaned, 'Methodology')),\n",
        "            ('Results', extract_section(cleaned, 'Results')),\n",
        "            ('Conclusion', extract_section(cleaned, 'Conclusion'))\n",
        "        ]\n",
        "\n",
        "        summaries = []\n",
        "        for title, content in sections:\n",
        "            if content:\n",
        "                print(f\"Summarizing {title}...\")\n",
        "                summary = summarize_large_text(content)\n",
        "                summaries.append(summary)\n",
        "\n",
        "        final_summary = clean_summary(\" \".join(summaries))\n",
        "        print(\"\\n===== Final Summary =====\\n\")\n",
        "        print(final_summary)\n",
        "\n",
        "summarize_button.on_click(on_summarize_clicked)\n",
        "\n",
        "# Display UI\n",
        "display(widgets.VBox([\n",
        "    widgets.Label(\"Select a research paper from your Google Drive folder (MyDrive/Papers):\"),\n",
        "    file_dropdown,\n",
        "    summarize_button,\n",
        "    output_area\n",
        "]))\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "07861c2f2ad845878569f1c02b2a0b1b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0c1009f2c8aa42a5bdfcc47477e0abdd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0edb57ccadf24698bdfcbce7a2f24e95": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "LabelModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_19bf33d480874cffbb69e62f7ecb91c1",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_f28eddaed9fd440a902ac4f52d47e0d7",
            "value": "Select a research paper from your Google Drive folder (MyDrive/Papers):"
          }
        },
        "112430cde5604dafa49909faf2662575": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "13139d9d16c74865bb95cd4f928f6aa3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_aaf7dc1b1a124ce78b83811f8dbe2f01",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_50f7e6c8a084407080f0678ef6b6cdc5",
            "value": "model.safetensors:â€‡100%"
          }
        },
        "185cfa7df6ff4698bad888e19fbbc730": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "19bf33d480874cffbb69e62f7ecb91c1": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "222ad09a47d44bab936ea5f62d2ba3bf": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "23c010d4894048c5b87bdc415652493d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e57a861bf6384786a0074e004e3e04c9",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_9d60e5493b774d50bea3a0dd09902458",
            "value": "â€‡1.21k/1.21kâ€‡[00:00&lt;00:00,â€‡65.6kB/s]"
          }
        },
        "261d05d789fa4acc9e6851d8fcaca0ba": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2ce80cc81fb54508b681b493e60c039a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a451f183441341e2be39d64d395ca0e2",
            "max": 1208,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_261d05d789fa4acc9e6851d8fcaca0ba",
            "value": 1208
          }
        },
        "2e8b32a739da4340bdb11763e70aa881": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f2fdb265266141e5b77b0855448b166d",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_cf6a055a513941aea062c046bd6425f6",
            "value": "config.json:â€‡100%"
          }
        },
        "3611bbf194e146b48db9d6f7d20d2e9a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3755aee31ec9464d8c2504c67f41d5f0": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3b1ef663e3ba4cec8c2556273f28fa61": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ButtonStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "46998388e97049bf9da4ac8f9a2ac332": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6eb75272f8ea473fa9c8ea9c0246922f",
              "IPY_MODEL_5a4e9c50dce1427bb4f529a61264a9e6",
              "IPY_MODEL_9739bbc0811f4ab6bb517382835b61e5"
            ],
            "layout": "IPY_MODEL_f86166a918e148f8a94cb90c84a6b0e2"
          }
        },
        "4d98ed10a05044718bed04064f84f872": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DropdownModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DropdownModel",
            "_options_labels": [],
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "DropdownView",
            "description": "Paper:",
            "description_tooltip": null,
            "disabled": false,
            "index": null,
            "layout": "IPY_MODEL_222ad09a47d44bab936ea5f62d2ba3bf",
            "style": "IPY_MODEL_f7bca2873c774210b306ca3c59b7caa0"
          }
        },
        "4fdad0235dbf4a289f52325bb99e67c1": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "50f7e6c8a084407080f0678ef6b6cdc5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "55ad16f59f8d4e4c845773706e924131": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5868b37780e943dfa52d03c15459f16d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5a4e9c50dce1427bb4f529a61264a9e6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d23fb01f8d1945acad691cdaec954858",
            "max": 791656,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a1d70cabb78b4547bf820ecb307834c0",
            "value": 791656
          }
        },
        "5ea2f2ba007d4833b7bc9bdd9067551c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "60a6773ba2f242f781d2946de1d84169": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_da707172215a404ebc0b4a40b83ff6f5",
            "max": 147,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ca483b91fc044266bdc33a95b531f6cd",
            "value": 147
          }
        },
        "6433558be09f443991be3d92f46b2389": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "67b7991404b14e07840612c17f5ad1dc": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6c6c357f264749c2b25af984743041af": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4fdad0235dbf4a289f52325bb99e67c1",
            "max": 891646390,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9b2bc9db24ce4db2b11ba02c4630e3ad",
            "value": 891646390
          }
        },
        "6eb75272f8ea473fa9c8ea9c0246922f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_70eab40fce164367adccfd1af7663a30",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_55ad16f59f8d4e4c845773706e924131",
            "value": "spiece.model:â€‡100%"
          }
        },
        "70eab40fce164367adccfd1af7663a30": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "714f1139efe64b04b7383b83278ce848": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d4b47d47f05547a690ecd9c8c6eb7472",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_be7889662a5f4ea2b5f68800b6332a09",
            "value": "tokenizer.json:â€‡100%"
          }
        },
        "754a7a6d8a8d49c4b2cdb11b9a07a6e0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9eb2ce2630e741718056d323cd33687b",
              "IPY_MODEL_60a6773ba2f242f781d2946de1d84169",
              "IPY_MODEL_f249b59a5009476e967bf122209b4642"
            ],
            "layout": "IPY_MODEL_c93ea600877144ca8d05e317f3a4ef4b"
          }
        },
        "8058233a09ad447588a3c6526f3514d5": {
          "model_module": "@jupyter-widgets/output",
          "model_module_version": "1.0.0",
          "model_name": "OutputModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_67b7991404b14e07840612c17f5ad1dc",
            "msg_id": "",
            "outputs": []
          }
        },
        "87d7fbc6c98e4d44bcc9566a9786413b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "93d197645aef41b2b4bb5f9960e1f520": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ba6941730413412085614c1d03de67b7",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_ba67de76f125460282e76812a71a2ba8",
            "value": "â€‡892M/892Mâ€‡[00:03&lt;00:00,â€‡278MB/s]"
          }
        },
        "9739bbc0811f4ab6bb517382835b61e5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fa564a314f7c4d1586c689d6bcd9865b",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_07861c2f2ad845878569f1c02b2a0b1b",
            "value": "â€‡792k/792kâ€‡[00:00&lt;00:00,â€‡4.01MB/s]"
          }
        },
        "998557bafeaa4b09af2c95ef12d66f2b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_13139d9d16c74865bb95cd4f928f6aa3",
              "IPY_MODEL_6c6c357f264749c2b25af984743041af",
              "IPY_MODEL_93d197645aef41b2b4bb5f9960e1f520"
            ],
            "layout": "IPY_MODEL_5868b37780e943dfa52d03c15459f16d"
          }
        },
        "9b2bc9db24ce4db2b11ba02c4630e3ad": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9d60e5493b774d50bea3a0dd09902458": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9eb2ce2630e741718056d323cd33687b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e14d8759f87b4de6b4c416e03d6299d5",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_87d7fbc6c98e4d44bcc9566a9786413b",
            "value": "generation_config.json:â€‡100%"
          }
        },
        "a05332cbae284043ae9fff6ca76013bb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5ea2f2ba007d4833b7bc9bdd9067551c",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_0c1009f2c8aa42a5bdfcc47477e0abdd",
            "value": "â€‡1.39M/1.39Mâ€‡[00:00&lt;00:00,â€‡6.92MB/s]"
          }
        },
        "a1d70cabb78b4547bf820ecb307834c0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a451f183441341e2be39d64d395ca0e2": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "aaf7dc1b1a124ce78b83811f8dbe2f01": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ba67de76f125460282e76812a71a2ba8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ba6941730413412085614c1d03de67b7": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "be7889662a5f4ea2b5f68800b6332a09": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c8752b07bb654518905783d0fbf406a1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_185cfa7df6ff4698bad888e19fbbc730",
            "max": 1389353,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6433558be09f443991be3d92f46b2389",
            "value": 1389353
          }
        },
        "c93ea600877144ca8d05e317f3a4ef4b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ca483b91fc044266bdc33a95b531f6cd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ccdb043398c345799e0ab9d385e6bff7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ButtonModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "success",
            "description": "Summarize",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_3611bbf194e146b48db9d6f7d20d2e9a",
            "style": "IPY_MODEL_3b1ef663e3ba4cec8c2556273f28fa61",
            "tooltip": ""
          }
        },
        "cf6a055a513941aea062c046bd6425f6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d23fb01f8d1945acad691cdaec954858": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d4b47d47f05547a690ecd9c8c6eb7472": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "da707172215a404ebc0b4a40b83ff6f5": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dd83c26adb6d417eba57a889e897f0e8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ddacbd8e66674ee380b8507f233021cb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e14d8759f87b4de6b4c416e03d6299d5": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e2b3aa92590247a99f345b864131848e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2e8b32a739da4340bdb11763e70aa881",
              "IPY_MODEL_2ce80cc81fb54508b681b493e60c039a",
              "IPY_MODEL_23c010d4894048c5b87bdc415652493d"
            ],
            "layout": "IPY_MODEL_e8fec43a27af4454bea3464c6c3b1d8f"
          }
        },
        "e57a861bf6384786a0074e004e3e04c9": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e8fec43a27af4454bea3464c6c3b1d8f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ea6ea5e0ebb44c7db15bbdf2c596e0f0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_714f1139efe64b04b7383b83278ce848",
              "IPY_MODEL_c8752b07bb654518905783d0fbf406a1",
              "IPY_MODEL_a05332cbae284043ae9fff6ca76013bb"
            ],
            "layout": "IPY_MODEL_112430cde5604dafa49909faf2662575"
          }
        },
        "eff3d18d41bb4b3e82df8b40be2481c5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "VBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0edb57ccadf24698bdfcbce7a2f24e95",
              "IPY_MODEL_4d98ed10a05044718bed04064f84f872",
              "IPY_MODEL_ccdb043398c345799e0ab9d385e6bff7",
              "IPY_MODEL_8058233a09ad447588a3c6526f3514d5"
            ],
            "layout": "IPY_MODEL_3755aee31ec9464d8c2504c67f41d5f0"
          }
        },
        "f249b59a5009476e967bf122209b4642": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dd83c26adb6d417eba57a889e897f0e8",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_ddacbd8e66674ee380b8507f233021cb",
            "value": "â€‡147/147â€‡[00:00&lt;00:00,â€‡7.92kB/s]"
          }
        },
        "f28eddaed9fd440a902ac4f52d47e0d7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f2fdb265266141e5b77b0855448b166d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f7bca2873c774210b306ca3c59b7caa0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f86166a918e148f8a94cb90c84a6b0e2": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fa564a314f7c4d1586c689d6bcd9865b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
